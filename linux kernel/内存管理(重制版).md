### 概述
```
// 内存管理涉及的领域
> 内存中的物理内存页的管理
> 分配大块内存的伙伴系统
> 分配较小内存的slab分配器
> 分配非连续内存块的vmalloc
> 进程地址空间

// linux将虚拟地址空间分为两部分: 顶部小的内核空间和底部大的用户空间
例如32位系统中，虚拟地址空间总长度为4G，每个进程虚拟地址空间是3G，内核空间只有1G

// 物理页(页帧)，虚拟页(页)

// 页表
虚拟地址空间映射到物理地址空间的结构叫做页表
32位系统4k大小的页，在虚拟地址空间4G的前提下，需要包含400万项的数组。在64位系统下数组会更大
因为虚拟地址空间大部分区域没有使用，没有关联到页帧，那么就可以使用功能相同但内存用量更少的模型：多级分页

// 多级分页
+-----+-----+-----+--------+
| PGD | PMD | PTE | offset |  虚拟地址
+-----+-----+-----+--------+

全局页表    中间页表    页表    页帧

页表的特点是对于不用的虚拟地址空间，不必创建中间页表和页表，与用单个数组的方法相比可以节省大量内存

// 加速多级访问
多级分页的缺点是必须访问多个数组才能完成虚拟地址到物理地址的转换，cpu提供两种方法进行加速
(1) MMU(内存管理单元)
    MMU内存控制单元，管理内存并把虚拟地址转换成物理地址的'硬件'
(2) TLB(地址转换后备缓冲器)
    地址转换中出现最频繁的那些地址存储在TLB中，这样就避免了对内存的频繁访问

// 内核与体系
将虚拟地址映射到物理地址，32位体系使用两级页表，64位体系需要四级页表
内核与体系结构无关的部分总是假定使用四级页表，对于达不到四级页表的体系，内核通过空页表对其补全

// 内存映射
内存映射是重要抽象手段(注意不是虚拟地址映射物理地址)
映射方法可以将任何来源的数据传输到进程的虚拟地址空间中，那么作为映射目标的地址空间区域，可以像访问内存一样对其读写

// 内存体系架构

        cpu     <--- QPI ---> cpu                   
        
        memory node           memory node

        pg_data_t             pg_data_t

        ZONE_DMA              ZONE_DMA
    +---ZONE_NORMAL           ZONE_NORMAL       
    |   ZONE_HIGNMEM          ZONE_HIGHMEM
    |
    |      free_area[]
    |   +- 2^0
    |   |  2^1 
    +-->|  2^2 ----+
        |  ...     |
        +- 2^10    |
                   |
    +--------------+
    |
    +--> free_list[]
            |
            |----------> list_head <--> pages <--> pages ... 
            |
            ...

```

### NUMA模型的内存组织
```
[IBM: Linux的NUMA技术] https://www.ibm.com/developerworks/cn/linux/l-numa/index.html
[深挖NUMA] http://www.litrin.net/2017/10/31/%E6%B7%B1%E6%8C%96numa/
[NUMA特性对MySQL性能的影响测试] https://cloud.tencent.com/developer/article/1159058

SMP系统中，传统的UMA模型，所有处理器都共享系统总线，因此当处理器的数目增大时，系统总线的竞争冲突加大，系统总线将成为瓶颈

NUMA技术有效结合了SMP系统易编程性和MPP(大规模并行)系统易扩展性的特点，较好解决了SMP系统的可扩展性问题，是为当今高性能服务器的主流体系结构之一

Linux在调度器、存储管理、用户级API等方面进行了大量的NUMA优化工作

1、NUMA架构
    不同的内存器件和CPU核心从属不同的Node，每个Node都有自己的集成内存控制器(IMC)
    在Node内部，架构类似SMP，使用IMC Bus进行不同核心间的通信
    不同的Node间通过QPI进行通信

            Node 0                         Node 1
    +---------------------+         +---------------------+
    | cpu  cpu   cpu  cpu |         | cpu  cpu   cpu  cpu |
    |  |    |     |    |  |         |  |    |     |    |  |
    |  +----+--+--+----+  |         |  +----+--+--+----+  |  ...
    |          | IMC bus  |         |          | IMC bus  |
    |   --+----+----+--   |         |   --+----+----+--   |
    |    内存 内存 内存    |         |    内存 内存 内存    |
    +---------------------+         +---------------------+
               |            QPI                |     
        -------+-------------------------------+--------

    > 一般来说一个内存插槽对应一个Node
    > Node内部的访问速度远高于之间的访问，QPI延迟大于IMC
    > 默认情况下，内核不会对页面进行Node之间的迁移

2、NUMA存储管理
    linux采用节点(Node 对应于上述Node)、区(Zone)和页(page)三级结构来描述物理内存的

    (1) 之间的关系
                                          +-> ZONE_DMA      -> zone_mem_map -> struct page -> ... 
        Node -> pg_data_t -> node_zones --+-> ZONE_NORMAL   -> zone_mem_map -> struct page -> ...
                                          +-> ZONE_HIGNMEM  -> zone_mem_map -> struct page -> ...

        > 内存划分为节点，每个节点关联一个处理器，在内核中用pg_data_t表示
        > 每个节点又划分为内存域(区)，一般由3个区组成(ZONE_DMA, ZONE_NORMAL, ZONE_HIGHMEM)

    (2) 节点
        typedef struct pglist_data {
            // 该结点的zone类型，一般包括ZONE_HIGHMEM、ZONE_NORMAL和ZONE_DMA三类
            zone_t node_zones[MAX_NR_ZONES];
            zonelist_t node_zonelists[GFP_ZONEMASK+1];
            int nr_zones;
            // 它是 struct page 数组的第一页，该数组表示结点中的每个物理页框
            struct page *node_mem_map;
            unsigned long *valid_addr_bitmap;
            struct bootmem_data *bdata;
            // 该结点的起始物理地址
            unsigned long node_start_paddr;
            unsigned long node_start_mapnr;
            unsigned long node_size;
            int node_id;
            struct pglist_data *node_next;
        } pg_data_t;

    (3) Zone
        每个结点的内存被分为多个块，称为zones，它表示内存中一段区域
        > zone的类型主要有ZONE_DMA、ZONE_NORMAL和ZONE_HIGHMEM
        > x86-64只有ZONE_DMA和ZONE_NORMAL
        > zone是用struct zone_t描述的，它跟踪页框使用、空闲区域和锁等信息
        > zone结构中包含交换的阈值，当系统中可用的内存比较少时，kswapd将被唤醒，并进行页交换
        > zone结构的访问非常频繁，会将其保存在cpu高速缓存中
        > 通过结构体中的ZONE_PADDING填充，使自旋锁处于自身的缓存行中

        详细描述见下

    (4) 内存分配策略
        当一个任务请求分配内存时，Linux采用局部结点分配策略
        > 首先在自己的结点内寻找空闲页
        > 如果没有，则到相邻的结点中寻找空闲页
        > 如果还没有，则到远程结点中寻找空闲页，从而在操作系统级优化了访存性能

3、NUMA调度器
    NUMA系统中，由于局部内存的访存延迟低于远地内存访存延迟，因此将进程分配到局部内存附近的处理器上可极大优化应用程序的性能

4、NUMA的策略
    1) 默认(default)
        内存分配给正在执行进程的CPU
        NUMA在默认在本地CPU上分配内存，会导致CPU节点之间内存分配不均衡，当某个CPU节点的内存不足会使用Swap而不是直接从远程节点分配内存
        内核中设置numa=off或者numactl --interleave=all来关闭这个特性

    2) 交叉(interleave)：在多个CPU上交织分配
    3) 绑定(bind)：绑定进程和内存到指定节点
    4) 优先(preferred)：优先在指定节点分配，失败再换节点

5、命令
    numactl [option]
    [option]
        --hardware              查看NUMA内存分组
        --interleave=nodes      在指定节点上交织分配
        --membind=nodes         在指定节点上分配内存
        --cpunodebind=nodes     绑定到指定CPU节点
        --physcpubind=cpus      绑定到指定CPU核心
        --localalloc            强制在本地节点分配内存
        --preferred=node        优先在指定节点分配内存

6、注意
    > 内核不会将内存页面从一个 NUMA Node 迁移到另外一个 NUMA Node
    > 在SMP环境下，NUMA采用对CPU和内存分组的方式管理资源，在默认策略情况下，CPU使用自己组内的内存空间，若新进程申请内存时，没有足够的空闲内存，则可能用到swap空间，而不去其他CPU组申请内存
    > 对于消耗内存很大的程序(redis)，要分配所有的node供其使用，命令 numactl --interleave=all redis

```

### Node节点
```
https://zhuanlan.zhihu.com/p/68473428[Linux中的物理内存管理 [二]]

```