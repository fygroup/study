
### 一致性
```
// 为什么需要一致性
    > 数据不能存在单个节点（主机）上，否则可能出现单点故障
    > 多个节点（主机）需要保证具有相同的数据
    一致性算法就是为了解决上面两个问题


// 分布式系统的事务处理与数据一致性
1、ACID
    事务是由一系列对数据进行访问与更新的操作所组成的一个程序执行逻辑单元，特指数据库事务
    当多个程序并发访问数据时，事务可以提供一个隔离方法，防止相互干扰
    事务具有四个特性：Atomicity（原子性）、Consistency（一致性）、Isolation（隔离性）、Durability（持久性）
    > Atomicity
        事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。
        事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。
    > Consistency
        事务开始前和结束后，数据库的完整性约束没有被破坏
        比如A向B转账，不可能A扣了钱，B却没收到。
    > Isolation
        同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰
        比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账
        隔离级别    脏读    可重复度    幻读
        未授权读取  存在    不可以      存在
        授权读取    不存在  不可以      存在
        可重复读取  不存在  可以        存在
        串行化      不存在  可以        不存在
    > Durability
        事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚
    
2、分布式事务
    在分布式领域，为了确保业务的完整与正确性，分布式事务无法避免，而且非常复杂
    
3、CAP
    在分布式系统中很难做到ACID，所以提出了CAP
    CAP理论主张基于网络的数据共享系统，都最多只能拥有以下三条中的两条
    Consistency（一致性）、Available（可用性）、Partition Tolerance (分区容错性)
    > 一致性
        指数据多个副本之间是否能够保持一致
        即更新操作成功并返回客户端后，所有节点在同一时间的数据完全一致，这就是分布式的一致性。一致性的问题在并发系统中不可避免，对于客户端来说，一致性指的是并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致
    > 可用性
        指系统提供的服务必须一直处于可用的状态，强调每一个操作总是能够在"有限时间内" "返回结果"
        好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况
    > 分区容错性
        即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务
        分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，对于用户而言并没有什么体验上的影响
        
    CAP三个特性只能满足其中两个，那么取舍的策略就共有三种：
    > CA without P (违背分布式)
        如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的
    > CP without A
        如果不要求A可用，相当于每个请求都需要在服务器之间保持强一致。而P(分区)会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统
        设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库
    > AP wihtout C
        要高可用并允许分区，则需放弃一致性，一旦分区发生，节点之间可能会失去联系。为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性
        典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A(可用性)方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞
    
4、BASE
    基于CAP理论发展而来，核心思想是即便不能达到强一致性，但可以根据应用特点采用适当的方式达到最终一致性
    BasicallyAvailable（基本可用）、Soft State（软状态）、Eventual consistent（最终一致性）
    > Basically Available
        假设系统，出现了不可预知的故障，但还是能用，相比较正常的系统而言
        响应时间上的损失、功能上的损失
    > Soft state
        硬状态是多个节点的数据副本都是一致的
        软状态允许系统在多个不同节点的数据副本存在数据延时
    > Eventually consistent
        保证数据最终一致性

```

### 一致性协议
```
[Paxos算法详解] https://zhuanlan.zhihu.com/p/31780743
[分布式系列文章——Paxos算法原理与推导] https://www.cnblogs.com/linbingdong/p/6253479.html

为了需要在系统可用性与数据一致性之间进行反复权衡，于是产生了一致性协议

1、2PC(二阶段提交)
    包含协调者和参与者
    协调者负责调度参与者的行为，并最终决定这些参与者是否把事务真正进行提交
    绝大多数关系型数据库采用二阶段提交协议来完成分布式事务处理
    (1) 协议
        1) 阶段一(提交事务请求)
            协调者组织参与者对事物投票表态阶段
            1> 事务询问
                协调者向所有参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待参与者响应
            2> 执行事务
                各参与者执行事务操作，并记录undo和redo信息
            3> 反馈响应
                事务执行成功的参与者反馈给协调者yes响应，表示事务可以执行。相反不能成功执行的参与者返给协调者no响应，表示事务不可以执行
        2) 阶段二(执行事务提交)
            参与者投票表明是否要继续执行事务提交操作，分为两种情况
            1> 执行事务提交
                假如协调者从所有参与者获得的反馈都是yes，则会执行事务提交
                协调者向所有参与者节点发出commit请求
                1> 事务提交
                    参与者收到commit请求后，会正式执行事务提交操作，然后释放事务执行期间占用的资源
                2> 反馈提交结果
                    参与者完成事务提交后，向协调者发送ACK消息
                3> 完成事务
                    协调者收到所有参与者的ACK后，完成事务
                
            2> 中断事务
                协调者向所有参与者节点发出commit请求后，假如任何一个参与者反馈no响应或等待超时后，则会中断事务
                1> 发送回滚请求
                    协调者向所有节点发出Rollback请求
                2> 事务回滚
                    参与者收到Rollback后，会利用undo执行事务回滚，完成回滚后释放资源
                3> 反馈事务回滚结果
                    参与者完成回滚后，向协调者发送ACK消息
                4> 中断事务
                    协调者收到所有参与者的ACK消息后，完成事务中断

    (2) 优缺点
        虽然上述协议原理简单，实现方便，但是存在以下缺点
        1) 同步阻塞
            二阶段提交过程中，所有参与该事务的操作都处于阻塞状态。各个参与者会存在等待其他参与者的情况，严重影响性能
        2) 单点问题
            一旦协调者出现问题，整个系统无法运行
        3) 数据不一致
            二阶段提交过程中，因为一些故障导致一部分参与者收到commit请求，而另一部分没有收到commit请求，会出现数据不一致的情况
        4) 太过保守
            没有容错机制，任意一个节点失败会导致整个事务失败

2、3PC
    3PC在2PC的第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前，各参与者节点的状态都一致
    同时在协调者和参与者中都引入超时机制，当参与者各种原因未收到协调者的commit请求后，会对本地事务进行commit，不会一直阻塞等待，解决了2PC的单点故障问题，但3PC 还是没能从根本上解决数据一致性的问题
    (1) 协议
        1) CanCommit
            协调者向所有参与者发送CanCommit命令，询问是否可以执行事务提交操作。如果全部响应YES则进入下一个阶段
        2) PreCommit
            协调者向所有参与者发送PreCommit命令，询问是否可以进行事务的预提交操作
            参与者接收到PreCommit请求后，如参与者成功的执行了事务操作，则返回Yes响应，进入最终commit阶段
            一旦参与者中有向协调者发送了No响应，或因网络造成超时，协调者没有接到参与者的响应，协调者向所有参与者发送abort请求，参与者接受abort命令执行事务的中断
        3) doCommit
            在前两个阶段中所有参与者的响应反馈均是YES后，协调者向参与者发送DoCommit命令正式提交事务，如协调者没有接收到参与者发送的ACK响应，会向所有参与者发送abort请求命令，执行事务的中断
            一旦参与者进入三阶段，由于协调者宕机或之间的网络出现问题时，参与者不会一直等待(阻塞)，超时后会自己进行commit
    (2) 优缺点
        1) 优点
            与二阶段相比最大优点是降低参与者的阻塞范围，并且在出现单点故障后继续达成一致
        2) 缺点
            参与者接收到preCommit后，如果网络出现分区，协调者和参与者无法进行正常通信，这是参与者依然会进行事务提交，必然导致数据不一致

3、Paxos
    Paxos算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复
    Paxos算法解决的问题正是分布式一致性问题，即一个分布式系统中的各个进程如何就某个值（决议）达成一致
    (1) 三个角色
        Proposer: 提出提案 (Proposal)。Proposal信息包括提案编号 (Proposal ID) 和提议的值 (Value)
        Acceptor：参与决策，回应Proposers的提案。收到Proposal后可以接受提案，若Proposal获得多数Acceptors的接受，则称该Proposal被批准。
        Learner：不参与决策，从Proposers/Acceptors学习最新达成一致的提案（Value）
    (2) Proposer与Acceptor
        1) Proposer生成提案
            1> Prepare请求
                Proposer生成一个新的提案编号N，然后向某个Acceptor集合的成员发送'编号为N的Prepare请求'，要求集合中的成员做出以下回应(承诺)
                > 不再批准任何编号小于N的提案
                > 如果Acceptor已经接受过提案，那么就向Proposer反馈当前已批准编号小于N的最大编号的提案
            2> Accept请求
                > 确定提案
                    如果Proposer收到了半数以上的Acceptor的响应，那么它就可以生成编号为N，value值为V的提案。这里的V是所有的响应中编号最大的提案的Value，N为当前Accept提案编号。如果半数以上的响应中都没有任何提案，那么此时V就可以由Proposer自己生成
                > 发送提案               
                    在此Proposer确定提案后，然后再将此提案(N, V)发给某Acceptor集合
        2) Acceptor批准提案
            Acceptor处理逻辑：一个Acceptor只接受编号大于已响应过编号的请求
            Acceptor会收到来自Proposer的两种请求Prepare和Accept
            1> 响应Prepare(N)请求，做出是否响应此编号
                如果编号N小于等于之前Acceptor承诺的编号，那么拒绝此响应。否则接受响应，并返回已批准的最大编号的提案(如果有)
            2> 响应Accept(N, V)请求，做出是否批准此提案
                如果编号N小于之前Acceptor承诺的编号，则拒绝此提案，否则接受此提案。并且承诺不接受编号小于等于的N的响应(prepare请求)
        
    (3) 算法两个阶段描述
        两个阶段可能轮回交替
        1) 阶段一
            1> Prepare
                Proposer生成全局唯一且递增的编号N，然后向所有Acceptor发送编号为N的Prepare请求(这里无需携带内容)
            2> Promise
                Acceptor收到编号为N的Prepare请求，做出"两个承诺，一个应答"
                > 两个承诺
                    > 不再接受任何编号小于等于N的Prepare请求
                    > 不再接受任何编号小于N的Propose提案
                > 一个应答
                    如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它批准过的编号最大的提案(注意此处是批准的提案，不是承诺的提案编号，说明已经历了Accept过程)作为响应反馈给Proposer，如果没有批准过提案，则返回空值
                
        2) 阶段二
            1> Propose
                如果Proposer收到"半数以上"的Acceptor对于其发出编号N的Prepare请求做出了响应，然后从收到的提案中选择编号最大的提案valueMax，将自身的编号(注意不是最大编号)和此值，即(N, valueMax)发送给所有的Acceptor。如果响应中不包含任何提案，那么就填上任意值
            2> Accept
                Acceptor收到Propose(N, valueMax)请求后，只要该Acceptor尚未对编号大于N的Prepare请求做出响应，那么接受此提案(持久化，发送给learner)。否则拒绝此提案

        伪代码描述:

                      Proposer(N, V)                              Acceptor(ResN, AcceptN, AcceptV)

                   |  Prepare(N)全局唯一，递增 -+                 
                   |                           |               
                   |                           +----------------> def Prepare(N):
            step1  |                                                 if N <= ResN:
                   |                                                    不响应或return(err)
                   |                                                 if N > ResN:
                   |                                                    ResN = N
                   |                                    +--------       return (ok, AcceptN, AcceptV) 或 (ok, null, null)
                                                        |
                   |  if 收到超半数的ok响应 <------------+
                   |       V
                   |       if 响应有提案:
                   |          V = 响应中获得最大提案编号对应的值
                   |       else:
                   |          V = 自己定义
                   |       发起Accept(N, V)请求
                   |  else:
                   |       重新获取N
            step2  |       重新发起Prepare(N)请求
                   |               |
                   |               +--------------------------->  def Accept(N, V):
                   |                                                 if N >= ResN:
                   |                                                     AcceptN = N
                   |                                                     AcceptV = V
                   |                                                     ResN = N
                   |                                                     return (ok)
                   |                                                 else:
                   |               +---------------------------          return (err)
                   |               ↓
                   |  收到超半数响应ok，说明已接受提案
                   |  否则，没有接受提案，重新Prepare


    (4) 活锁
        假设有两个Proposer一次提出编号递增的提案，最终会陷入死循环(无法保证活性)
        > Proposer P1发出编号为M1的Prepare请求，收到过半ok响应。完成阶段一流程
        
        > Proposer P2发出编号为M2(M2>M1)的Prepare请求，也收到过半ok响应。也完成阶段一流程，此时Acceptor承诺不再接受编号小于M2的提案

        > P1进入第二阶段，发出Accept(M1)请求，Acceptor拒绝请求(M1<M2)，P1重新发出编号M3(M3>M2)的Prepare请求，Acceptor收到请求后承诺不再接受编号小于M3的提案

        > P2进入的二阶段，发出Accept(M2)请求，Acceptor拒绝请求(M2<M3)，P2又重新Prepare请求

        > 进入死循环

        解决方案：选取一个主Proposer，只有主Proposer才能提出议案(Prepare)


    (5) Learner方案
        Learner学习（获取）被选定的value有如下三种方案
        1) Acceptor接受一个提案，然后发送给所有Learner
            优点：Learner能尽快的获取被选定的提案
            缺点：通信次数高(m*n)

        2) Acceptor接受一个提案，然后发送给主Learner，主Learner再通知其他Learner
            优点：通信次数少(m+n-1)
            缺点：主Learner出现故障

        3) Acceptor接受一个提案，然后发送给部分Learner，部分Learner再通知其他Learner
            优点：可靠性好
            缺点：太复杂
```

### Chubby
```
paxos协议的实现

1、系统结构
    (1) Master
        一个Chubby集群(通常5台服务器)，采用Paxos选择一台master，master租期期间不会有其他服务器成为master，master会不断续租，直到master出现故障，新一轮选举，产生新master
    (2) 数据存储方式
        集群中每个服务器都维护一份数据库副本，但只有master才有写数据库的权限，其他服务器使用Paxos协议从master服务器上同步数据
    (3) 定位Master
        请求DNS服务器获得所有Chubby服务器列表，轮询该列表。如果是非Master服务器则会返回Master地址
    (4) 请求的发送
        客户端会将所有请求发给Master
        对于写请求，Master会采用Paxos将其广播给集群中的所有服务器，在收到过半响应后，在返回给客户端
        对于读请求，Master不会进行广播处理，则直接返回给客户
    (5) 服务器故障处理
        master故障会重新选举
        非master故障，整个集群不会停止，此崩溃服务器会在恢复后自动加入Chubby集群中去
        新加入的服务器，需要先同步Chubby最新数据库，更新DNS列表，然后加入到正常的Paxos运作流程中
    (6) 服务器地址变更
        在Chubby运行过程中，Master会周期的轮询DNS列表。当感知到服务器地址列表变更时，Master会将集群数据库中的地址列表作相应的变更，然后其他服务器通过复制的方式获得最新的服务器地址列表     

2、目录与文件
    Chubby提供了一套与Unix文件系统非常相近的但更简单的访问接口
    > 数据结构
        Chubby数据结构可看作有文件和目录组成的树
        例如   /ls/foo/wombat/pouch
        > ls代表所有节点的前缀，代表锁服务
        > foo是指定的集群的名字，从DNS可以查询由一个或多个服务器组成该Chubby集群
        > 剩余部分路径/wombat/pouch 是一个真正包含业务含义的节点名字，由Chubby服务器内部解析并定位到数据节点
    > 数据节点
        每个数据节点分为持久节点和临时节点
        持久节点需要显示调用API来进行删除
        临时节点会在客户端失效后自动删除，临时节点的生命周期和客户端会话绑定
    > 数据节点的元数据信息
        每个数据节点包含元数据
        > 实例编号
            用于标识创建数据节点的顺序
        > 文件内容编号
            只针对文件，用于标识文件内容的变化情况，在文件被写入时编号增加
        > 锁编号
            用于标识节点锁状态变更情况，当节点锁从自由状态转换成持有状态时，该编号增加
        > ACL编号
            ACL是访问控制列表，ACL编号用于标识节点ACL信息变更情况，当ACL信息被写入时该编号增加
        > 校检码
            Chubby还会标识一个64位的文件内容校检码，以便客户端能够识别文件是否变更

3、锁与序列器
    1) 读写锁
        在Chubby中任何一个数据节点都能充当读写锁来使用
        Chubby舍弃了严格的强制锁，即客户端可以在没有获取任何锁的情况下访问Chubby文件
    3) 锁延迟和锁序列器
        这两种策略用来解决由于消息延迟和重排序引起的分布式锁问题(详见下)
        > 锁延迟
            由于异常导致的锁释放，服务器会保留一段时间，其他客户端无法获得此锁
            锁延迟可很好的解决网络闪断导致的断开
            此方案不完美
        > 锁序列器
            一个锁序列器包括锁的名字、锁模式(读、写)、锁序号
            当带有锁的请求到达服务器时，服务器会先检查锁序号是否有效，以及检查是否处于恰当的锁模式来决定是否拒绝此请求

4、事件通知机制
    客户端获得Chubby服务端状态不是通过轮询来获得的，而是通过注册事件通知，常见事件如下：
    文件内容变更
    节点新增和删除
    Master转移

5、缓存
    1) 作用
        为了避免客户端频繁的请求服务端，客户端会存在缓存来存储相关信息（文件内容和元数据信息）。但是会带来系统复杂性，最主要就是缓存一致性
    2) 缓存一致性
        > 原理
            通过租期机制来保存缓存一致性
            Master通过向每个客户端发送过期信息来保证客户端数据一致性，会导致客户端要么从缓存中得到一致性数据，要么访问出错
        > 解决方法
            > 客户端缓存会存在一个租期，一旦到期需要向服务器续订租期来维护一致性
            > 当要修改数据时，Chubby服务端会阻塞该修改操作，然后Master向'所有'缓存该数据的客户端,发送缓存过期信号使其失效
            > 等到Master收到所有客户端的应答时(客户端允许缓存过期或要求更新缓存)，再继续进行之前的修改操作
            > 总之Chubby是通过强一致性来解决此问题的。尽管性能开销比较大，但是很稳

6、KeepAlive
    Chubby客户端与服务端的通信都是基于TCP来进行操作的

    待续。。。
        
```

### Zookeeper
```
Zookeeper是一个典型的分布式数据一致性解决方案
分布式应用程序可以基于它实现 数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁、分布式队列等功能

特点
    顺序一致性
    原子性
    单一视图
    可靠性
    实时性

设计目标
    简单的数据模型
        服务器内存中的一个数据模型，有一系列的称为ZNode数据节点组成
        Znode之间的层级关系就像文件系统的目录结构一样
    构建集群
        Zookeeper集群(3~5台服务器)的每台机器都会在内存中维护当前的服务器状态，并且之间维持通信
        只要超一半机器正常那么此集群正常
        Zookeeper客户端与集群的任意一台机器相连，一旦断开后，会自动连接到其他机器
    顺序访问
        来自客户端的请求，Zookeeper都会分配全局唯一的递增编号
        此编号反映了所有事务操作的先后顺序，应用程序可以使用这个特性实现同步原语
    高性能
        全部数据存储在内存中

基本概念
    集群角色
        分为Leader, Follower, Observer三种角色，非主从模式
        > 通过选举产生一台Leader
        > Leader提供'读写'服务
        > Follower和Observer提供'读'服务
        > Observer机器不参与Leader选举，也不参与'过半写成功策略'

    session
        Session 会话 客户端启动会与服务端建立一个 TCP 长连接，通过这个连接可以发送请求并接受响应，以及接受服务端的 Watcher 事件通知
        第一次建立连接开始，客户端会话生命周期也开始

    数据节点
        > 数据模型
            数据模型的单元称为数据节点(ZNode)
            数据模型是一棵树(ZNode Tree)，由'/'分割路径，就是一个ZNode，例如/foo/path1
            每个ZNode上保存自己的内容和一系列属性信息
        > 分类
            > 持久节点
                ZNode一旦创建会一直保存在Zookeeper上，除非主动删除
            > 临时节点
                生命周期和客户端会话绑定

    版本
        对应于每个ZNode，Zookeeper会维护一个Stat数据结构
        Stat记录了这个ZNode的三个数据版本
        version     当前Znode的版本
        cversion    当前Znode子节点的版本
        aversion    当前Znode的ACL版本
    
    Watcher
        事件监听器，Zookeeper的一个重要特性
        Zookeeper允许用户在'指定节点'上注册一些Watcher，并且在一些特定事务触发时，Zookeeper服务端会将事件通知到'之前指定'的节点上

    ACL
        Zookeeper采用ACL策略来进行权限控制，类似UNIX文件系统的权限控制，定义以下权限
        CREATE  创建子节点权限
        READ    获取节点数据和子节点列表权限
        WRITE   更新节点数据权限
        DELETE  删除子节点权限
        ADMIN   设置节点ACL权限

```

### ZAB 协议
```
[Zookeeper——一致性协议:Zab协议] https://www.jianshu.com/p/2bceacd60b8a

                                            client
                                               ↑
                                   Propose     |       Propose
                                <---------     |    ---------->  
                                   Ack         ↓       Ack
    client <---------> Follower ---------->  Leader <---------- Follower <---------> client
                                   Commit     ↑ ↑      Commit            
                                <----------   | |   ---------->
                                              | |
                                              | |
    client <---------> Observer <-------------+ +-------------> Observer <---------> client


1、概念
    ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持崩溃恢复和原子广播协议
    (1) Leader和Follower
        主备模式的系统架构来保持集群中各个副本之间数据一致性
        所有客户端写入数据都是写入到主进程(Leader)中，然后由Leader复制到备份进程(Follower)中。从而保证数据一致性

    (2) 两个模式
        整个 Zookeeper 就是在这两个模式之间切换
        1) 崩溃恢复
            当leader服务器出现网络中断、崩溃退出和重启等异常情况，ZAB协议会进入恢复模式，并'选举出新的Leader'
        2) 消息广播
            当选举出新的Leader时，同时集群中过半的机器与该leader完成状态'同步'后，ZAB协议会退出崩溃恢复模式

2、消息广播
    类似一个二阶段提交过程
    (1) 步骤
        1) Leader将client的请求转化为proposal提案发送给所有的Follower
            Leader 服务器将客户端的请求转化为事务 Proposal 提案，同时为每个 Proposal 分配一个全局的ID，即zxid。
            Leader 服务器为每个 Follower 服务器分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中，并且根据 FIFO 策略进行消息发送
            > 队列
                Leader 服务器与每一个 Follower 服务器之间都维护了一个单独的 FIFO 消息队列进行收发消息，使用队列消息可以做到异步解耦
            > zxid
                ZXID是64位数字，高32位代表Leader周期的epoch编号，低32代表事务递增计数器

        2) Leader收到半数以上Follower的ACK反馈，然后向所有Follower提交(commit)任务
            Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息
            Leader 接收到超过半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功可以发送 commit 消息

        3) Follower收到commit消息，将前一个Proposal提交
            Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交

3、崩溃恢复
    一旦 Leader 服务器出现崩溃或者由于网络原因导致 Leader 服务器失去了与过半 Follower 的联系，那么就会进入崩溃恢复模式
    (1) 两种异常
        1) 一个事务在 Leader 上提交了，并且过半的 Folower 都响应 Ack 了，但是 Leader 在 Commit 消息发出之前挂了。
        2) 假设一个事务在 Leader 提出之后，Leader 挂了

    (2) 两个保证
        如果发生上述两种情况，ZAB协议必须满足以下承诺
        1) 已经在Leader上提交的事务最终被所有服务器都提交
        2) 确保丢弃那些只在Leader服务器上提出的Proposal事务
    
    (3) Leader的选举
        Leader需要满足以下条件
        1) 新选举出来的Leader不能包含未提交的Proposal
            即新选举的Leader必须都是已经提交了Proposal的Follower服务器节点
        2) 新选举的Leader节点中含有最大的zxid
            这样做的好处是可以避免Leader服务器检查Proposal的提交和丢弃工作

3、数据同步
    (1) 同步
        当崩溃恢复之后，在正式工作之前
        1) 确认
            Leader服务器会首先确认事务日志中的所有的Proposal是否已经被集群中过半的服务器Commit
        2) 添加
            当所有的Follwer服务器都成功同步之后，Leader会将这些服务器加入到可用服务器列表中
    (2) 丢弃(Porposal)
        zxid = Leader周期编号 + 事务编号
        每当选举产生一个新的Leader，就会从这个Leader服务器上取出本地事务日志充最大编号Proposal的zxid，并从zxid中解析得到对应的 epoch 编号，然后再对其加1
        当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的 ZXID 进行比对，比对结果要么回滚，要么和 Leader 同步

```

### zookeeper的节点数据操作和应用场景
```
1、zookeeper的保证
    > 更新请求顺序进行，来自同一个每年uuu8三个年，client的更新请求按其发送顺序依次执行
    > 数据更新原子性，一次数据更新要么成功，要么失败
　　> 全局唯一数据视图，client无论连接到哪个server，数据视图都是一致的
　　> 实时性，在一定事件范围内，client能读到最新数据

2、节点数据操作流程
    (1) 读请求
        客户端连接到集群中某一节点，读请求，直接返回
    (2) 写请求
        1) 写入请求直接发送到Leader节点时的操作
            1> Client向Leader发出写请求
            2> Leader将数据写入到本节点，并将数据发送到所有的Follower节点
            3> 等待Follower节点返回
            4> 当Leader接收到一半以上节点(包含自己)返回写成功的信息之后，返回写入成功消息给client
        2) 写入请求发送到Follower节点的操作步骤
            1> Client向Follower发出写请求
            2> Follower节点将请求转发给Leader
            3> Leader将数据写入到本节点，并将数据发送到所有的Follower节点
            4> 等待Follower节点返回
            5> 当Leader接收到一半以上节点(包含自己)返回写成功的信息之后，返回写入成功消息给原来的Follower
            6> 原来的Follower返回写入成功消息给Client

3、应用场景
    zookeeper应用在开源项目HBase, Spark, Flink, Storm, Kafka, Dubbo

    (1) 数据发布/订阅
        一个常见的场景是配置中心，发布者把数据发布到 ZooKeeper 的一个或一系列的节点上，供订阅者进行数据订阅，达到动态获取数据的目的
        > 配置信息一般有几个特点
            数据量小的KV
            数据内容在运行时会发生动态变化
            集群机器共享，配置一致
        > 推拉结合的模式
            客户端向服务端注册自己关注的节点，一旦该结点发生改变，服务端就会向相应的客户端发送watcher事件通知，客户端收到这个消息通知后，会主动到服务端获得最新数据

    (2) 负载均衡
        负载均衡是一种手段，用来把对某种资源的访问分摊给不同的设备，从而减轻单点的压力
        每台WorkServer启动的时候都会到Server(zookeeper)创建临时节点
        每台ClientServer启动的时候，都会到Server(zookeeper)节点下面取得所有WorksServer节点，并通过一定算法取得一台并与之连接。

    (3) 命名服务
        > 提供类 JNDI 功能，可以把系统中各种服务的名称、地址以及目录信息存放在 ZooKeeper，需要的时候去 ZooKeeper 中读取
        > 制作分布式的序列号生成器
            // 不用UUID，没有规律，难以理解
            ZooKeeper 可以生成有顺序的容易理解的同时支持分布式环境的编号

    (4) 分布式协调/通知
        基于zookeeper临时节点的特性，不同机器在 ZooKeeper 的一个指定节点下创建临时子节点，不同机器之间可以根据这个临时节点来判断客户端机器是否存活
        // 分布式机器间通信方式
            1) 心跳检测
                定时PING、TCP长连接固有的心跳检测
            2) 工作进度汇报
            3) 系统调度

    (5) 集群管理
        传统基于给每个服务器上部署一个Agent，这个Agent周期性的向监控中心汇报自己的机器状态
        但是弊端很明显：升级困难、统一的Agent无法提供业务耦合紧密的监控、编程语言多样性

        zookeeper利用自己的两大特性(watcher、临时节点)可以实现集群机器存活监控，例如
        > 自动化的线上运维
            通常在新增机器的时候，需要首先将指定的 Agent 部署到这些机器上去
            Agent 部署启动之后，会首先向 ZooKeeper 的指定节点进行注册(创建一个临时子节点)
        > 上线通知
            当Agent 在 ZooKeeper 上创建完这个临时子节点后，对 /machines 节点关注的监控中心就会接收到“子节点变更”事件，即上线通知，于是就可以对这个新加入的机器开启相应的后台管理逻辑
        > 下线通知

    (6) Master选举
        分布式系统中的Master用来协调其他单元，例如读写分离的场景通常由Master处理的
        确保Master挂了之后的重新选举尤为重要

    (7) 分布式锁
        [基于 Zookeeper 的分布式锁实现] https://cloud.tencent.com/developer/article/1408101

        1) 排他锁
            排他锁又称'写锁'
            如果事务 T1 对数据对象 O1 加上了'排他锁'，那么加锁期间，只允许事务 T1 对 O1 进行读取和更新操作
            核心是保证当前'有且仅有一个事务获得锁'，并且锁释放后，所有正在等待获取锁的事务都能够被通知到

            > 定义锁
                一个节点就可定义为一个锁
            > 获得锁
                所有客户端都会通过调用 create() 接口尝试在 /x_lock 创建临时子节点 /x_lock/lock
                最终只有一个客户端创建成功，那么该客户端就获取了锁
                同时没有获取到锁的其他客户端，注册一个子节点变更的 Watcher 监听
            > 释放锁
                获取锁的客户端发生宕机或者正常完成业务逻辑后，就会把临时节点删除
                临时子节点删除后，其他客户端又开始新的一轮获取锁的过程

        2) 共享锁
            共享锁又称'读锁'
            如果事务 T1 对数据对象 O1 加上了'共享锁'，那么当前事务 T1 '只能'对 O1 进行'读取操作'，'其他事务'也只能对这个数据对象'加共享锁'，直到数据对象上的所有共享锁'都被释放'

            > 定义锁
                zookeeper上的znode表示一个锁，/s_lock/[HOSTNAME]-请求类型-序号
                // 例如
                /s_lock
                    |----- /host1-R-000000001
                    |----- /host2-R-000000002
                    |----- /host3-W-000000003
                    |----- /host4-R-000000004
                    |----- /host5-R-000000005
                    |----- /host6-R-000000006
                    `---- /host7-W-000000007

            > 获得锁
                客户端会在/s_lock这个节点下面创建一个临时节点，如果是'读'请求创建类型为'R'，'写'请求创建类型为'W'
                例如：/s_lock/192.168.0.1-W-00000001

            > 判断读写顺序
                不同事务可以对同一个对象进行读取操作，而更新操作必须在当前没有任何事务进行读写操作的情况下进行
                1> 创建完节点后，获取 s_lock 的所有子节点，并对该节点(/s_lock)注册'子节点变更'的 Watcher 监听
                2> 然后确定自己的节点序号在所有的子节点中的顺序
                3> 判断读写
                    > 对于读请求
                        如果没有比自己小的子节点，或所有比自己序号小的子节点都是读请求，表明自己已经成功获得共享锁，并开始读操作
                        如果比自己序号小的子节点有写请求，则进入等待
                    > 对于写请求
                        如果自己不是序号最小的子节点，那么需要等待
                4> 等待的客户端收到watcher通知后，重复步骤1>

            > 释放锁
                获取锁的客户端发生宕机或者正常完成业务逻辑后，就会把临时节点删除
                临时子节点删除后，'其他客户端(所有子客户端)'收到/s_lock 子节点变动的通知，又开始新的一轮获取锁的过程

        3) 惊群效应
            上述共享锁在判断读写顺序的时候，当有子节点移除的时候，后面的子节点对应的客户端都会收到watcher通知，这会对集群造成很大的冲击
            // 解决
            调用 getChildren 接口的时候获取到所有已经创建的子节点列表，但是这个时候不要注册任何的 Watcher
            当无法获取共享锁的时候，调用 exist() 来对比自己小的那个节点注册 Wathcer
            读请求: 在比自己序号小的最后一个写请求节点注册 Watcher
            写请求: 向比自己序号小的最后一个节点注册 Watcher

    (6) 分布式队列
        1) FIFO
            使用 ZooKeeper 实现 FIFO 队列，入队操作就是在 queue_fifo 下创建自增序的子节点，并把数据（队列大小）放入节点内。出队操作就是先找到 queue_fifo 下序号最下的那个节点，取出数据，然后删除此节点
            /queue_fifo
                |----- /host1-000000001
                |-----  /host2-000000002
                |-----  /host3-000000003
                `----  /host4-000000004

            创建完节点后，根据以下步骤确定执行顺序
            1> 通过 get_children() 接口获取 /queue_fifo 节点下所有子节点
            2> 通过自己的节点序号在所有子节点中的顺序
            3> 如果不是最小的子节点，那么进入等待，同时向比自己序号小的最后一个子节点注册 Watcher 监听
            4> 接收到 Watcher 通知后重复1>

        2) Barrier
            使用场景：并行操作需要等待所有并行操作完成时，才能进行下一步操作
            /queue_barrier 节点是一个已经存在的默认节点，并且将其节点的数据内容赋值为一个数字 n 来代表 Barrier 值
            比如 n=10 代表只有当 /queue_barrier 节点下的子节点个数达到10才会打开 Barrier
            > 所有线程开始
                所有的线程启动时在 ZooKeeper 节点 /queue_barrier 下插入顺序临时节点，然后检查 /queue_barrier 下所有 children 节点的数量是否为所有的线程数，如果不是，则等待，如果是，则开始执行
                1>  getData() 获取 /queue_barrier 节点的数据内容
                2> getChildren() 获取 /queue_barrier 节点下的所有子节点，同时注册对子节点列表变更的 Watcher监听
                3> 统计子节点的个数，如果子节点个数不足10，那么进入等待
                4> 接收 Watcher 通知后，重复2>

            > 所有线程结束
                 所有线程在执行完毕后，都检查 /queue_barrier 下所有 children 节点数量是否为0，若不为0，则继续等待
            
```

### etcd
```
[Raft算法原理] https://www.codedump.info/post/20180921-raft/
[Raft图解] http://thesecretlivesofdata.com/raft/

1、架构
    1) HTTP Server
        用于处理用户发送的 API 请求以及其它 etcd 节点的同步与心跳信息请求。
    2) Store
        用于处理 etcd 支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是 etcd 对用户提供的大多数 API 功能的具体实现。
    3) Raft
        Raft 强一致性算法的具体实现，是 etcd 的核心。
    4) WAL
        Write Ahead Log（预写式日志），是 etcd 的数据存储方式
        除了在内存中存有所有数据的状态以及节点的索引以外，etcd 就通过 WAL 进行持久化存储
        WAL 中，所有的数据提交前都会事先记录日志。Snapshot 是为了防止数据过多而进行的状态快照；Entry 表示存储的具体日志内容

2、应用场景
    (1) 服务发现
        在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接
    (2) 分布式通知与协调
        与消息发布和订阅相似，用到了 etcd 中的 Watcher 机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理
    (3) 负载均衡
        为了保证服务的高可用以及数据的一致性，通常都会把数据和服务部署多份，以此达到对等服务，即使其中的某一个服务失效了，也不影响使用。由此带来的坏处是数据写入性能下降，而好处则是数据访问时的负载均衡
    (4) 分布式锁
        锁服务有两种使用方式，一是保持独占，二是控制时序
    (5) 分布式队列
    (6) 集群监控与 Leader 竞选

3、部署
    
```