### 分布式键值系统
```
分布式键值模型可以看成是'分布式表格模型的一种特例'
由于它只支持针对单个key-value的增、删、查、改操作
```

### 名词解释
```
1、P2P
    对等网络(P2P)技术
    (1) C/S模式
        C/S模式中，数据的分发采用专门的服务器，多个客户端都从此服务器获取数据
        优点: 数据的一致性容易控制，系统也容易管理
        缺点: 单一失效点，资源限制，可扩展性差
    (2) P2P
        在P2P网络中，每个节点既可以从其他节点得到服务，也可以向其他节点提供服务
    (3) 组织结构
        1) DHT结构(分布式哈希表)
        2) 树形结构
        3) 网状结构

2、Gossip 协议
    redis使用的一致性协议，该协议无中心节点

```

### Amazon Dynamo
```
// 数据分布
    > 一致性Hash(DHT)
        一个token是一个虚拟节点
        给系统中每个节点分配一个随机token，这些token构成一个哈希环
        执行数据存放操作时，先计算主键的哈希值，然后存放到顺时针方向第一个大于或者等于该哈希值的token所在的节点
    > 异构型问题
        由于节点之间运算存在差异
        每个物理节点根据其性能的差异分配多个token
    > 自动负载均衡
        有3个节点 1(1, 4, 7), 2(2, 3, 8), 3(0, 5, 6)
        假设增加节点4，集群会分别将节点1和节点3的token 1和token 5迁移到节点4
        此时的节点token分配情况变为：1(4, 7), 2(2, 3, 8), 3(0, 6), 4(1, 5)    
    > Gossip
        Dynamo无中心节点
        每个节点维护整个集群的信息(用于token节点和物理节点的定位)
        所有节点每隔固定时间(比如1s)通过Gossip协议的方式从其他节点中任意选择一个与之通信的节点。如果连接成功，双方交换各自保存的集群信息。
        新加入的节点首先与种子节点通信

// 一致性与复制
    为了处理节点失效的情况(DHT环中删除节点)，需要对节点的数据进行复制
    > 数据备份
        假设数据存储N份，DHT定位到的数据所属节点为K，则数据存储在节点K, K+1 ... K+N-1上
        如果第K+i(0≤i≤N-1)台机器宕机，则往后找一台机器K+N临时替代
    > 数据回传
        如果k+i重启，上面临时替代的机器K+N通过Gossip协议发现，它会将这些临时数据归还K+i
    > NWR
        一种一致性协议
        N表示复制的备份数，R指成功读操作的最少节点数，W指成功写操作的最少节点数
        当W+R>N的时候，整个系统对于客户端来讲能保证强一致性
        多读少些：设置W=N,R=1
        读写平衡：设置N=3，W=2，R=2
        注意：没有成功操作的节点会通过向量时钟来保证一致性
    > 向量时钟
        向量时钟用一个[nodes,counter]对表示
        nodes表示节点，counter是一个计数器


// 容错
    > 异常
        临时性的异常和永久性异常
    > 数据回传
        针对临时性异常
    > Merkle树同步
        针对永久性异常
        如果超过了时间T机器K+i还是处于宕机状态，这种异常是永久性的
        每个非叶子节点对应多个文件，为其所有子节点值组合以后的哈希值
        叶子节点对应单个数据文件，为文件内容的哈希值
        这样任意数据不匹配将导致根节点不同
    > 读取修复
        读取操作如果发现了某些副本版本太老，则启动异步的读取修复任务

// 负载均衡
    > 随机分配
        每台物理节点加入时根据其配置情况随机分配S个Token
    > 数据范围等分+随机分配

// 读写流程
    > 写数据
        > 根据一致性哈希算法计算出每个数据副本所在的存储节点，其中一个副本作为本次写操作的协调者
        > 协调者并发地往所有其他副本发送写请求，每个副本将接收到的数据写入本地，协调者也将数据写入本地
        > 当某个副本写入成功后，回复协调者
        > 如果发给某个副本的写请求失败，协调者会将它加入重试列表不断重试。等到W-1个副本回复写入成功后（即加上协调者共W个副本写入成功），协调者可以回复客户端写入成功
        > 协调者回复客户端成功后，还会继续等待或者重试，直到所有的副本都写入成功。
    > 读数据
        > 根据一致性哈希算法计算出每个副本所在的存储节点，其中一个副本作为本次读操作的协调者
        > 协调者根据负载策略选择R个副本，并发地向它们发送读请求
        > 每个副本读取本地数据，协调者也读取本地数据
        > 等到R个副本回复读取成功后，协调者可以回复客户端
            > 如果R个副本返回的数据完全一致，将某个副本的读取结果回复客户端
            > 否则，根据修改时间戳选择最新的数据，当然用户也可以自定义冲突处理方法。
        > 读取过程中如果发现某些副本上的数据版本太旧，Dynamo内部会异步发起一次读取修复操作
```

### 淘宝Tair
```
https://www.jianshu.com/p/ccb17daed766

由一个中心控制节点和若干个服务节点组成

中心控制节点称为Config Server，服务节点称为Data Server

Config Server负责管理所有的Data Server，维护其状态信息，采用主备保证可靠性
Data Server对外提供各种数据服务，并以心跳的形式将自身状况汇报给Config Server


// 四种引擎
    在单机引擎可以使用
    非持久化的mdb、rdb；持久化的kdb、ldb
    分别基于四种开源的key/value数据库：memcached, Redis, Kyoto Cabinet和leveldb
    统一的API，无论底层使用何种引擎，上层的API是一样的

// Version
    Tair中的每个数据都包含版本号，版本号在每次更新后都会递增
    可以防止由于数据的并发更新导致的问题
    每一个value更像一个数组

// Config Server
    > 构建并存储数据分布对照表
    > 为client提供数据分布表的查询服务
        client会cache对照表，减少与config server的交互
    > 调度DataServer之间的数据迁移、复制
    > 与DataServer保持心跳来获知集群中存活节点的信息
    > 流程
        > configserver维护的对照表有一个版本号，每次新生成表，该版本号都会增加
        > 当有数据节点状态发生变化(新增或移除)时，configserver会根据当前可用的节点重新生成对照表，并通过数据节点的心跳，将新表同步给数据节点
        > 当客户端请求数据节点时，数据节点每次都会将自己的对照表的版本号放入response中返回给客户端，客户端接收到response后，会将数据节点返回的版本号和自己的版本号比较，如果不相同，则主动和configserver通信，请求新的对照表。
        > 所以客户端也不需要和configserver保持心跳，用于更新对照表
        
                version比较
        client <-----------> DataServer

        if version旧了
                    获得最新对照表            
            client <-------------> ConfigServer
            获得新的DataServer节点

                数据操作    
        client <--------> DateServer


// Data Server
    负责物理存储
    具有抽象引擎层，自由添加不同引擎
    根据ConfigServer构建的对照表完成数据的复制和迁移工作
    DataServer还有一个插件容器，可以动态地加载/卸载插件

// 数据分布
    采用一致性哈希，对于所有的key，分到Q个桶中
    桶是负载均衡和数据迁移的基本单位
    Config Server按照一定的策略把每个桶指派到不同的Data Server上
    保证桶分布的均衡性，就能够保证数据分布的均衡性
    > 负载均衡原则
        > 每个桶必须有n份拷贝
        > 一个桶的各份拷贝数据不能在同一个节点

// 对照表
    第一列是hash值，第二列是Data Server节点，例如有两个节点192.168.10.1和192.168.10.2
    0   192.168.10.1
    1   192.168.10.2
    2   192.168.10.1
    3   192.168.10.2
    4   192.168.10.1
    5   192.168.10.2

// 多备份的对照表
    2个备份的对照表如下，第二列为主节点，第三列为辅节点
    0 192.168.10.1 192.168.10.2
    1 192.168.10.2 192.168.10.1
    2 192.168.10.1 192.168.10.2
    3 192.168.10.2 192.168.10.1
    4 192.168.10.1 192.168.10.2
    5 192.168.10.2 192.168.10.1


// 容错
    自动的复制和迁移，当数据写入一个节点时，主节点根据对照表自动将数据写入到其他备份节点
    当某台Data Server故障不可用时，Config Server能够检测到

// 节点发生变化
    > 新增节点
        新增节点会被Config Server发现，然后重新构建对照表，构建依据以下两个原则：
        > 数据在新表中均衡地分布到所有节点上
        > 尽可能地保持现有的对照关系
    > 节点不可用
        ConfigServer依次判断该节点上每个桶
        > 如果该桶是辅副本，那么configserver会重新为该桶指定一个辅节点(如果是持久化存储，还将复制数据到新的辅节点上)
        > 如果该桶是主副本，那么configserver首先将该桶的辅节点提升为主节点，对外提供服务，并指定一个新的辅节点，确保数据的备份数

// 数据迁移
    节点的变化或负载不均衡会导致数据迁移，迁移的过程中需要保证对外服务
    假设数据(桶)由节点A迁往节点B，客户访问节点A
    > 当访问的数据没有迁移(A中)，直接返回数据
    > 当访问的数据已经迁移(B中)，A把请求发给B，B返回给客户
    > 当访问的数据正在迁移(A->B)
        如果是修改操作
        1) 修改A中的日志
        2) 等数据迁移完毕后，还要迁移日志到B中
        3) B通过日志进行修改操作

// 插件容器
    ConfigServer配置插件，然后将其同步给各个数据节点
    在DataServer中，插件分为request和response两类，可以分别在request和response时执行相应的操作，比如在put前检查用户的quota信息等

```