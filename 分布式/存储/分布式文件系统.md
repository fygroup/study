### Google File System
```
它构建在廉价的普通PC服务器之上，支持自动容错
GFS内部将大文件划分为大小约为64MB的数据块（chunk），并通过主控服务器（Master）实现元数据管理、副本管理、自动负载均衡等操作

GFS是Google分布式存储的基石，google其他存储系统直接或间接构建在GFS之上

1、系统架构
    > 三种角色
        GFS Master（主控服务器）、GFS ChunkServer（CS，数据块服务器）以及GFS客户端

    > 主控服务器Master
        维护了系统的元数据
            命名空间(Name Space)，也就是整个文件系统的目录结构以及chunk基本信息
            文件到chunk之间的映射
            chunk副本的位置信息，每个chunk通常有三个副本
        负责整个系统的全局控制(chunk租约管理、垃圾回收无用chunk、chunk复制等)

    > 数据块chunk
        GFS以chunk大小(64MB)来划分文件，每个chunk又以Block为单位(64KB)进行划分每个Block对应一个32位的校验和
        主服务器在创建时分配一个64位全局唯一的chunk句柄
        ChunkServer将chunk以linux文件的形式存放在磁盘中
        chunk的元信息包括全局唯一的ID、版本号、每个副本所在的ChunkServer编号、引用计数等
        为了确保可靠性，chunk在不同的机器上复制三分

    > 客户端
        提供给应用程序的访问接口
        客户端访问GFS时，首先访问主控服务器节点，获取与之进行交互的CS(ChunkServer)信息，然后直接访问这些CS，完成数据存取工作
        客户端不缓存文件数据(不需要缓存，文件太大且多变)，只缓存主控服务器中获取的元数据
        注意
            > MapReduce客户端使用方式为顺序读写，没有缓存文件数据的必要
            > Bigtable作为分布式表格系统，内部实现了一套缓存机制
            > 如何维护客户端缓存与实际数据之间的一致性是一个极其复杂的问题

2、关键问题
    (1) 租约机制
        > 租约的授权
            为了减轻Master的压力，GFS系统通过租约lease机制将chunk'写操作的权利'授权给ChunkServer
        > 主ChunkServer、备ChunkServer
            拥有租约的ChunkServe称为主ChunkServer，其他副本所在的ChunkServer称为为备ChunkServer
        > 租约有效期
            租约的有效期比较长(比如60秒)，只要没有出现异常，主ChunkServer可以不断向Master请求延长租约的效期直到整个chunk写满，然后再向Master申请
        > 版本号机制
            GFS对每个chunk维护一个版本号，每次给chunk进行租约授权或者主ChunkServer重新延长租约有效期时，Master会将chunk的版本号加1
        > 垃圾回收
            对于更新失败的chunk，版本号过低导致被Master标记为可删除
            Master的垃圾回收任务会定时检查，并通知ChunkServer将标记的chunk删除
        注意
            > 可写操作
                写操作可能不是按照 client -> 主ChunkServer -> 备ChunkServer 的方式发送数据流的
                而是主ChunkServer向备ChunkServer发送'可写操作'，然后数据流可能是其他ChunkServer发送的
            > Master在租约上面有两个功能
                > 授权给ChunkServer的写权限
                > 给新的chunk的版本号加一，之后的数据io操作由主chunkserver管理
    (2) 追加流程
        GFS主要是为了追加(append)而不是改写(overwrite)而设计的
        一方面是因为改写的需求比较少，主要是因为顺序写增加性能
        高效支持记录追加对基于GFS实现的分布式表格系统Bigtable是至关重要的，也是最复杂的

        步骤
        1) client请求Master
            client向Master请求chunk每个副本所在的位置，其中主ChunkServer持有修改租约
            如果没有ChunkServer持有租约(没有主ChunkServer)，说明该chunk最近没有写操作，Master会发起一个任务，按照一定的策略将chunk的租约授权给其中一台ChunkServer
        2) Master返回chunk的位置信息
            master返回chunk的主chunkserver和备chunkserver信息，client将缓存这些信息
        3) 发送副本
            1> 方法
                > 流水线发送
                    流水线操作用来减少延时
                    当一个ChunkServer(无论主还是备)接收到一些数据，它就立即开始转发
                    由于采用全双工网络，立即发送数据并不会降低接收数据的速率
                > 分离数据流与控制流的方法
                    主要是为了优化数据传输，每一台机器都是把数据发送给网络拓扑图上'最近'的尚未收到数据的数据
                > 基于主备复制方法
                    先把数据发给主ChunkServer，然后主ChunkServer再发给备ChunkServer
            2> 两种流程
                1> GFS流程
                    采用'分离数据流与控制流的方法'和'流水线'，充分利用网络拓扑，实现复杂
                    从近到远发送副本
                    > 客户端将要追加的记录发送到每一个副本，每一个ChunkServer会在内部的LRU结构中缓存这些数据
                    > 当所有副本都确认收到了数据，客户端发起一个写请求控制命令给主副本
                    > 由于主副本可能收到多个客户端对同一个chunk的并发追加操作，主副本将确定这些操作的顺序并写入本地
                    > 主副本把写请求提交给所有的备副本(每一个备副本会根据主副本确定的顺序执行写操作)
                    > 备副本成功完成后应答主副本
                    > 主副本应答客户端，如果有副本发生错误，将出现主副本写成功但是某些备副本不成功的情况，客户端将重试

                    +<--> 客户端 <---> 主控服务器
                    |       ↓ (date flow)  
          (control) |     备副本A <------->+
                    |       ↓ (date flow) |
                    +<--> 主副本 <-------->+ (control)
                            ↓ (date flow) | 
                          备副本B <------->+

                2> MapReduce流程
                    采用'主备复制方法'和'流水线'，实现简单
                    先发送给主副本，然后主副本再通过流水线的方式发送给备副本
                    > Client将待追加数据发送到主副本(chunkserver)，主副本可能收到多个客户端的并发追加请求，需要确定操作顺序，并写入本地
                    > 主副本将数据通过流水线的方式转发给所有的备副本
                    > 每个备副本收到待追加的记录数据后写入本地，所有副本都在本地写成功并且收到后一个副本的应答消息时向前一个副本回应

                    客户端 <--> 主控服务器
                      ↑↓  
                    主副本
                      ↑↓
                    备副本A
                      ↑↓
                    备副本B
        
        需要考虑的问题
        1) 追加的过程中可能出现主副本租约过期而失去chunk修改操作的授权
        2) 主副本或者备副本所在的ChunkServer出现故障

    (3) 容错机制
        1) Master容错
            > 日志操作
                GFS Master的修改操作总是先记录操作日志，然后修改内存，当Master发生故障重启时，可以通过磁盘中的操作日志恢复内存数据结构
            > 定期转存元数据
                为了减少Master宕机恢复时间，Master会定期将内存中的元数据以checkpoint文件的形式转储到磁盘中，从而减少回放的日志量
                注意：
                    Master持久化'命名空间'及'文件到chunk之间的映射'的元数据
                    对于'chunk副本的位置信息'的元数据，可以不进行持久化，因为ChunkServer保存了这些信息，Master重启时可以从他们获得此类数据
            > 热备元数据
                所有的元数据修改操作都必须保证发送到实时热备才算成功
            > Master备机
                如果Master宕机，还可以秒级切换到实时备机继续提供服务。为了保证同一时刻只有一台Master(依赖chubby进行选主操作)
        2) ChunkServer容错
            > 多副本
                GFS采用复制多个副本的方式实现ChunkServer的容错
                对于每个chunk，必须将所有的副本'全部写入成功'，才视为成功写入
                Master自动恢复其'丢失或不可恢复的副本'
            > 数据维持校验
                ChunkServer会对存储的数据维持校验和
                每个chunk又以Block为单位进行划分，每个Block对应一个32位的校验和
                读取chunk副本时会将读取的数据和校验和进行比较，如果不匹配，客户端将选择其他ChunkServer上的副本

3、设计策略
    (1) Master设计
        1) Master内存占用
        2) 负载均衡
            GFS中副本的分布策略需要考虑多种因素，如网络拓扑、机架分布、磁盘利用率等
            创建chunk副本
                新副本所在的ChunkServer的磁盘利用率低于平均水平
                限制每个ChunkServer'最近'创建的数量
                每个chunk的所有副本不能在同一个机架
            Master会定期扫描当前副本的分布情况，如果发现磁盘使用量或者机器负载不均衡，将执行重新负载均衡操作
            注意
                限制重新复制和重新负载均衡任务的拷贝速度，否则可能影响系统正常的读写服务
                新加入集群的chunkserver需要限制同时迁入的chunk数量防止被压垮
        3) 垃圾回收
            GFS采用延迟删除的机制
            1> 将元数据中的文件名改为隐藏(并且包含一个删除时间戳)
            2> Master定时检查，如果发现文件删除超过一段时间（默认为3天，可配置），那么它会把文件从内存元数据中删除
            3> 以后ChunkServer和Master的心跳消息中，Master会回复在Master元数据中的状态
            4> ChunkServer会释放这些chunk副本已经不存在的chunk信息
        4) 快照(snapshot)
            '快照'只是增加GFS中chunk的引用计数，表示这个chunk被快照文件引用了，等到客户端修改这个chunk时，才需要在ChunkServer中拷贝chunk的数据生成新的chunk，后续的修改操作落到新生成的chunk上

    (2) ChunkServer设计
        ChunkServer管理的是64M的chunk块
        1) 均匀分布
            ChunkServer要保证chunk均匀的分布在不同的磁盘中
        2) 删除变成移动
            删除大文件消耗时间太长，所以删除文件只需将chunk移动到回收站即可，新建的时候可以重用
    
```         

### Taobao File System
```
// blob文件的特点
    文档、图片、视频一般称为Blob数据，存储Blob数据的文件系统也相应地称为Blob存储系统
    每个Blob数据一般都比较大，而且多个Blob之间没有关联
    Blob文件系统的特点是数据写入后基本都是只读，很少出现更新操作

// TFS需要考虑的
    (1) 元数据的存储
        图片数量巨大，单机存放不了所有的元数据信息
    (2) 减少图片读取的IO次数
        多个逻辑图片文件共享一个物理文件

// TFS和GFS
    架构上借鉴了GFS，但与GFS又有很大的不同
    (1) TFS内部不维护文件目录树，每个小文件使用一个64位的编号表示
    (2) TFS是一个读多写少的应用，相比GFS,TFS的写流程可以做得更加简单有效

1、系统架构
    > NameServer(Master)
        两个NameServer节点(一主一备)
        NameServer通过心跳对DataSrver的状态进行监测
    > DataServer(ChunkServer)
        每个DataServer上会运行多个dsp进程
        一个dsp对应一个挂载点，这个挂载点一般对应一个独立磁盘，从而管理多块磁盘
    > block(chunk)
        将大量的小文件合并成一个大文件，这个大文件称为块(Block)
        每个Block拥有在集群内唯一的编号
        Block存储在DataServer中，大小一般为64MB，默认存储三份
    > 文件确定
        通过＜块ID，块内偏移＞可以唯一确定一个文件

2、追加流程
    TFS中的追加流程相比GFS要简单有效很多
    TFS是写少读多的应用，可以大大简化了系统的设计
    同一时刻每个Block只能有一个写操作，多个客户端的写操作会被串行化
    1) DataServer的确定
        NameServer需要根据DataServer上的可写块、容量和负载加权平均来选择一个可写的Block
        并且在该Block所在的多个DataServer中选择一个作为写入的主副本和备副本
    2) 写数据
        客户端向主副本写入数据，主副本将数据同步到多个备副本
    3) 通知NameServer
        所有副本修改成功后，主副本会首先通知NameServer更新Block的版本号，成功以后才会返回客户端操作结果
    4) 返回客户端
        小文件在TFS中的Block编号以及Block偏移
        应用系统会将这些信息保存到数据库中
    // 相比GFS,TFS的写流程不够优化
        1) 每个写请求都需要多次访问NameServer
        2) 数据推送也没有采用流水线方式减小延迟
        
3、NameServer
    1) 功能
        Block管理，包括创建、删除、复制、重新均衡
        DataServer管理，包括心跳、DataServer加入及退出
        管理Block与所在DataServer之间的映射关系
        注意
            不需要保存文件目录树信息，也不需要维护文件与Block之间的映射关系
    
    2) 管理DataServer
        NameServer与DataServer之间保持心跳
        如果NameServer发现某台DataServer发生故障，需要执行Block复制操作
        如果新DataServer加入，NameServer会触发Block负载均衡操作
        和GFS类似，TFS的负载均衡需要考虑很多因素，如机架分布、磁盘利用率、DataServer读写负载等
        新DataServer加入集群时也需要限制同时迁入的Block数量防止被压垮

4、几个问题
    (1) 图片去重
        需要部署一套文件级别的去重系统
        > 计算指纹
            采用MD5或者SHA1等Hash算法为图片文件计算指纹
        > 用指纹判断重复
            图片写入TFS之前首先到去重系统中查找是否存在指纹，如果已经存在，基本可以认为是重复图片
        > 存储指纹
            图片写入TFS以后也需要将图片的指纹以及在TFS中的位置信息保存到去重系统中
    (2) 图片删除
        > 只回收block
            每个Block中只要还有一个有效的图片文件就无法回收，也无法对Block文件进行重整
            如果系统的更新和删除比较频繁，需要考虑磁盘空间的回收
```

### 内容分发网络
```
// CDN通过将网络内容发布到靠近用户的边缘节点，使不同地域的用户在访问相同网页时可以就近获取
// 这样既可以减轻源服务器的负担，也可以减少整个网络中的流量分布不均的情况，进而改善整个网络性能

1、内容分发网络(CDN访问流程)

              提交网站域名        发出解析请求          重定向
        客户端----------->本地DNS------------>DNS服务器------>智能DNS负载均衡系统
          ↑                                                      ↓ 
          +-------------------返回边缘节点IP<---------------------+
        
        获得边缘节点IP后

        客户端 <-----> 边缘节点 <------> 源服务器

        请求成功，边缘节点会将页面缓存下来，下次用户访问时可以直接读取，而不需要每次都访问源服务

2、CDN架构
    
    L1-cache  L1-cache  L1-cache ...     50T

    L2-cache  L2-cache  L2-cache ...     100T

    图片服务器  图片服务器  图片服务器 ...  300T   
    
            TFS集群 TFS集群                         

    > 图片服务器
        每台图片服务器是一个运行着Nginx的Web服务器，它还会在本地缓存
    > 单个CDN节点
              客户端
        
            LVS     LVS

         Haproxy   Haproxy

              统一调度

        squid squid squid squid...

        源服务器 源服务器 源服务器 ...

        > 每个CDN节点内部通过LVS+Haproxy的方式进行负载均衡
        > 用squid服务器用来缓存Blob图片数据，如果没有命中就发送给源服务器
        > 数据通过一致性哈希的方式分布到不同的Squid服务器

3、硬件上的改变
    (1) 分级存储
        在Squid服务器上使用SSD+SAS+SATA混合存储
        图片随着热点变化而迁移，最热门的存储到SSD，中等热度的存储到SAS，轻热度的存储到SATA
    (2) 低功耗服务器
        CDN缓存服务是IO密集型而不是CPU密集型的服务，因此，选用Intel Atom CPU定制低功耗服务器

4、需要考虑的问题
    > 一致性
        CDN也是一种缓存，需要考虑与源服务器之间的一致性
        源服务器更新或者删除了Blob数据，需要能够比较实时地推送到CDN缓存节点，否则只能等到缓存中的对象被淘汰，而对象的有效期一般很长，热门对象很难被淘汰
    
```

