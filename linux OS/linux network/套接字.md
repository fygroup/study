### 套接字地址
```
// 字节序
    小端/大端         大端
    处理器字节序 ---> 网络字节序

    #include <arpa/inet.h>
    uint32_t htonl(uint32_t hostint32)  返回 网络字节序32位整数
    uint16_t htons(uint16_t hostint16)  返回 网络字节序16位整数
    uint32_t ntonl(uint32_t netint32)   返回 主机字节序32位整数
    uint16_t ntons(uint16_t netint16)   返回 主机字节序16位整数

1、通用socket地址
    不同的地址格式必须转换为此格式
    #include <sys/socket.h>
    struct sockaddr {
        sa_family_t  sa_family;     //地址族 unsigned short, AF_xxx
        char         sa_data[14];   //14字节 包含套接字中的目标地址和端口信息     
    }
    // 大小16个字节
2、专用socket地址
    (1) IPv4
        #include<netinet/in.h>
        typedef uint16_t in_port_t;
        typedef uint32_t in_addr_t;
        struct sockaddr_in {    
            sa_family_t     sin_family;     // 地址族 AF_INET
            in_port_t       sin_port;       // 16位端口号
            struct in_addr  sin_addr;       // 32位IP地址
            unsigned char   sin_zero[8];    // Same size as struct sockaddr，补齐剩余的字符
        }
        struct in_addr {
            in_addr_t       s_addr           // 32位IPv4地址；A.B.C.D 
        }

    (2) IPv6
        struct sockaddr_in6 { 
            sa_family_t     sin6_family;    // AF_INET6
            in_port_t       sin6_port;      // 16位端口号
            uint32_t        sin6_flowinfo;  // IPv6 flow information
            struct in6_addr sin6_addr;      // IPv6 address
            uint32_t        sin6_scope_id;  // 
        }
        struct in6_addr { 
            unsigned char   s6_addr[16];    // 128位IPv6地址长度；XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX
        }

    (3) unix域套接字地址
        #include <sys/un.h>
        struct sockaddr_un {
			sa_family_t     sun_family;     // AF_UNIX
            char            sun_path[108];  // pathname
        }

3、addr转换
    (1) tcp套接字转换
		struct sockaddr_in my_addr;
		my_addr.sin_family      = AF_INET;
		my_addr.sin_port        = htons(80);                 //uint16转换成网络字节序
		my_addr.sin_addr.s_addr = inet_addr("192.168.2.201") //inet_addr将字符串转换为网络字节序，inet_ntoa则将网络字节序转换为字符串
		bzero(&(my_addr.sin_zero), 8);                       //sin_zero置0
		struct sockaddr* myaddr = (struct sockaddr*)&my_addr //转换成sockaddr

	(2) unix域套接字转换
		struct sockaddr_un un;
		memset(&un, 0, sizeof(un));
		un.sun_family = AF_UNIX;
		strcpy(un.sun_path, "foo.socket");
		struct sockaddr *myaddr = (struct sockaddr*)&un;
	
	// socket绑定addr
	if((fd = socket(AF_UNIX, SOCK_STREAM, 0)) < 0)
		err_sys("socket failed");
	if(bind(fd, myaddr, sizeof(myaddr)) < 0)
		ERR_EXIT("bind");
```

### TCP套接字相关函数
```
1、socket函数
    #include <sys/socket.h>
    int socket(int domain, int type, int protocol)
    // domain(域)
		AF_INET     IPv4协议
		AF_INET6    IPv6协议
		AF_LOCAL    UNIX域协议
		AF_ROUTE	路由套接字
		AF_KEY		秘钥套接字

    // type
		SOCK_STREAM     字节流协议(有序、可靠、双向、面向连接字节流，tcp)
		SOCK_DGRAM	    数据报协议(固定长度、无连接、不可靠报文传递，udp)
		SOCK_RAM        ip协议数据报接口(又称为原始套接字)，用于直接访问网络层，绕过传输层(tcp、udp)，需要超级用户特权
        SOCK_CLOEXEC    在fork exec时，关闭子进程复制的socket
                        https://cloud.tencent.com/developer/article/1177094
        SOCK_NONBLOCK   非阻塞型套接字（除了此处可以设置非阻塞，还可以fcntl函数的O_NONBLOCK设置非阻塞）

    // protocol    	0(表示为给定的域和套接字选择默认协议)

2、bind函数
    // 将地址绑定在套接字上
    int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
	// 返回值	为0时表示绑定成功，-1表示绑定失败，errno查看错误值
	// errno值
		EADDRINUSE	给定地址已经使用	
		EBADF	sockfd不合法	
		EINVAL	sockfd已经绑定到其他地址	
		ENOTSOCK	sockfd是一个文件描述符，不是socket描述符	
		EACCES	地址被保护，用户的权限不足
	// 注意
		bind可以不指定ip和port，系统会为其指定
		如果内核为套接字选择一个临时的端口号，函数并不会返回其端口号，可以调用getsockname查看端口号

3、connect
    #include <sys/socket.h>
    ing connect(int sockfd, const struct sockaddr *servaddr, socklen_t addrlen);
	// sockfd	客户端套接字描述符
	// servaddr	包含服务器IP地址和端口号的套接字地址结构
	// addrlen	套接字地址结构的大小
	// 返回值	为0时表示绑定成功，-1表示绑定失败，errno查看错误值
	// errno值
		ETIMEDOUT	未收到响应则返回该错误(内核会先发送一个SYN，无响应则等待6s后再发送一个，若仍无响应则等待24s后再发送一个。共等待了75s)
		ECONNREFUSED	服务器对客户响应一个RST(1、主动取消；2、不存在连接)
		EHOSTUNREACH/ENETUNREACH	某个路由发送ICMP错误(地址不可达)
	// 注意
		调用connect前不必非得调用bind，如果没有bind，内核会确定源IP并选择一个临时端口作为源端口
		connect将激发TCP三路握手过程，函数会阻塞进程，直到成功或出错才返回
		connect失败则该套接字不可再用，必须关闭，不能对这样的套接字再次调用connect函数。必须close后重新调用socket
	
4、listen
	int listen(int sockfd, int backlog);
	// backlog	规定了内核应为相应套接字排队的最大连接个数，监听套接字维护两个队列(未完成连接队列、已完成连接队列)，两个队列之和不超过backlog
	// 返回值	成功返回0，失败返回-1
	// 注意
		此函数指示内核应接受指向该套接字的连接请求
		SYN到达时，如果队列已满，TCP忽略该SYN分节，不能返回RST，因为客户无法区分'连接错误'和'队列已满'
		
5、accept
    https://www.cnblogs.com/wangcq/p/3520400.html

    TCP服务器端依次调用socket()、bind()、listen()之后，就会监听指定的socket地址了
    TCP客户端依次调用socket()、connect()之后就向服务器发送了一个连接请求
    TCP服务器监听到这个请求之后，就会调用accept()函数取接收请求，这样连接就建立好了。之后就可以开始网络I/O操作了，即类同于普通文件的读写I/O操作

    int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
    // sockfd 	服务器的socket描述字
    // addr		客户端的协议地址
    // addrlen	第三个参数为协议地址的长度
    // 注意
		三次握手发生在这一步
		内核为每个由服务器进程接受的客户连接创建了一个已连接socket描述字，当服务器完成了对某个客户的服务，相应的已连接socket描述字就被关闭

6、close函数
	关闭套接字，并终止tcp连接
	#include <unistd.h>
	int close(int sockfd);
	// 返回值	成功0，错误-1
	// 注意
		close是把套接字标记为关闭，立即返回调用进程，然后TCP将尝试发送已排队等待发送到对端的任何数据，发送完毕后再开始TCP终止序列
		close会将套接字描述符的引用计数减1，如果引用计数仍大于0，则不会引起TCP的四次挥手终止序列

7、shutdown函数
	#include <sys/socket.h>
	int shutdown(int sockfd, int how);
	// sockfd	需要关闭的socket的描述符
	// how
		SHUT_RD(0)		关闭sockfd上的读功能，此选项将不允许sockfd进行读操作
		SHUT_WR(1)		关闭sockfd的写功能，此选项将不允许sockfd进行写操作
		SHUT_RDWR(2)	关闭sockfd的读写功能
	// 注意
		shutdown不管引用计数就能激发TCP的正常连接终止序列
		shutdown可以只关闭一个方向的数据传送，close终止读和写两个方向的数据传送
		
8、getsockname、getpeername
	获得套接字关联的本地地址
	int getsockname(int sockfd, struct sockaddr* localaddr, socklen_t* addrlen);
	获得套接字关联的外地地址
	int getpeername(int sockfd, struct sockaddr* peeraddr, socklen_t* addrlen);

```

### UDP
```
```

### TCP SCTP
```
```

### setsockopt套接字选项
```
1、函数
	int setsockopt(int socket, int level, int option_name,const void *option_value, socklen_t option_len);
	int getsockopt(int socket, int level, int option_name, void *option_value, socklen_t *option_len);

	// socket		套接字
	// level		level级别，一般设置SOL_SOCKET
	// option_name	设置的选项
		不同的级别，设置不同的选项
		(1) SOL_SOCKET
			SO_BROADCAST    bool    使用广播方式传送
			SO_DEBUG        bool    记录调试信息
			SO_DONTROUTE    bool    绕过外出路由表查询，直接传送，送出的数据包不要利用路由设备来传输
			SO_KEEPALIVE    bool    周期测试连接是否存活
			SO_LINGER       struct linger*  若有数据待发送(关闭时有未发送数据)，则延迟关闭
			SO_RCVBUF       int     接收缓冲区大小
			SO_SNDBUF       int     发送缓冲区大小
			SO_RCVLOWAT     int     接收缓冲区低水位标记，接收缓冲区中数据大于其标记时，I/O复用系统调用将通知应用程序可以从对应的socket上读数据
			SO_SNDLOWAT     int     发送缓冲区低水位标记，发送缓冲区中数据低于其标记时，I/O复用系统调用将通知应用程序可以从对应的socket上写数据
			SO_RCVTIMEO     timeval 接收超时
			SO_SNDTIMEO     timeval 发送超时
			SO_REUSEADDR    bool    允许重用本地地址
			SO_REUSEPORT    bool    允许重用本地端口
			SO_OOBINLINE    bool    在常规数据流中接收带外数据，当接收到OOB数据时会马上送至标准输入设备
		(2) IPPROTO_IP
		(3) IPPROTO_TCP
			TCP_MAXSEG      bool    TCP最大分节大小(MSS)
			TCP_NODELAY     bool    禁止Nagle算法
	// option_value	代表欲设置的值
	// option_len	则为option_value的长度
	// 返回值		成功则返回0, 错误返回-1, 错误原因存于errno

2、SO_REUSEADDR和SO_REUSEPORT
	https://zhuanlan.zhihu.com/p/35367402

	(1) SO_REUSEADDR
		当服务端出现timewait状态的链接时，确保server能够重启成功

	(2) SO_REUSEPORT(linux v3.9 引入)
		1) 场景
			对于多进程/多线程场景，每个进程/线程都有一个独立的socket，并且bind相同的ip:port，独立的listen()和accept()
			提高服务器的接收链接的并发能力(每个进程或线程都要设置SO_REUSEPORT)
			例如Nginx新版本是多进程同时监听同一个ip:port(每个进程bind同一个ip:port，但是只有一个进程会得到响应)
		2) 解决的问题
			> 避免了应用层多线程或者进程监听同一ip:port的"惊群效应"
			> 内核层面实现负载均衡，保证每个进程或者线程接收均衡的连接数
			> 只有effective-user-id相同的服务器进程才能监听同一ip:port(安全性考虑)
				
	(3) 示例
		struct sockaddr_in addr;
		memset(&addr, 0, sizeof(addr));
		addr.sin_family = AF_INET;
		addr.sin_addr.s_addr = inet_addr("0.0.0.0");
		addr.sin_port = htons(9980);

		void work () {
			int listenfd = socket(AF_INET, SOCK_STREAM, 0);
			if (listenfd < 0) _exit(-1);
			int ret = 0;
			bool reuse = TRUE;
			ret = setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR,(const void *)&reuse , sizeof(int));
			if (ret < 0) _exit(-1);
			ret = setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT,(const void *)&reuse , sizeof(int));
			if (ret < 0) _exit(-1);        
			ret = bind(listenfd, (struct sockaddr *)&addr, sizeof(addr));
			if (ret < 0) _exit(-1);
			ret = listen(listenfd, 10);
			if (ret < 0) _exit(-1);
			struct sockaddr clientaddr;
			int len = 0;
			while(1) {
				int clientfd = accept(listenfd, (struct sockaddr*)&clientaddr, &len);
				if (clientfd < 0) _exit(-1);
				close(clientfd);
			}
		}
		
		int main(){
			int i = 0;
			for (i = 0; i< 6; i++) {
				pid_t pid = fork();
				if (pid == 0) {
					work();
				}
				if(pid < 0) {
					perror("fork");
					continue;
				}
			}
			int status,id;
			while((id=waitpid(-1, &status, 0)) > 0);
			if(errno == ECHILD) {
				printf("all child exit\n");
			}
			return 0;
		}

3、一些设置的实例
	(1) 是否重用该socket
		bool bReuseaddr = TRUE;
		setsockopt(s,SOL_SOCKET ,SO_REUSEADDR,(const char*)&bReuseaddr,sizeof(bool));

	(2) 如果要已经处于连接状态的socket在调用close后强制关闭，不经历TIME_WAIT的过程
		bool bDontLinger = FALSE;
		setsockopt(s,SOL_SOCKET,SO_DONTLINGER,(const char*)&bDontLinger,sizeof(bool));

	(3) 发送或接收超时设置
		int nNetTimeout=1000;   // 1s
		//发送时限
		setsockopt(socket, SOL_S0CKET,SO_SNDTIMEO, (char *)&nNetTimeout,sizeof(int));
		//接收时限
		setsockopt(socket, SOL_S0CKET,SO_RCVTIMEO, (char *)&nNetTimeout,sizeof(int));

	(4) 发送或接收缓冲区设置
		在send()的时候，返回的是实际发送出去的字节(同步)或发送到socket缓冲区的字节(异步)。系统默认的状态发送和接收一次为8688字节(约为8.5K)，
		在实际的过程中发送数据和接收数据量比较大，可以设置socket缓冲区，而避免了send(),recv()不断的循环收发
		// 接收缓冲区
		int nRecvBuf=32*1024;   //设置为32K
		setsockopt(s,SOL_SOCKET,SO_RCVBUF,(const char*)&nRecvBuf,sizeof(int));
		//发送缓冲区
		int nSendBuf=32*1024;   //设置为32K
		setsockopt(s,SOL_SOCKET,SO_SNDBUF,(const char*)&nSendBuf,sizeof(int));

	(5) 发送接收缓冲区设置为0
		发送数据时，系统缓冲区的内容拷贝到socket缓冲区
		接收数据时，socket缓冲区的内容拷贝到系统缓冲区
		为了避免上述拷贝而影响性能(???)
		int nZero=0;
		setsockopt(socket, SOL_S0CKET, SO_SNDBUF, (char *)&nZero,sizeof(nZero));
		setsockopt(socket, SOL_S0CKET, SO_RCVBUF, (char *)&nZero, sizeof(int));

	(6) 一般在发送UDP数据报的时候，希望该socket发送的数据具有广播特性
		bool bBroadcast=TRUE;
		setsockopt(s,SOL_SOCKET,SO_BROADCAST,(const char*)&bBroadcast,sizeof(BOOL));

	(7) 如果在发送数据的过程中，而调用了close()后可以让关闭停留一段时间
		struct linger {
			u_short l_onoff;
			u_short l_linger;
		};
		linger m_sLinger;
		m_sLinger.l_onoff=1;	// 1：允许逗留，2：不允许逗留
		m_sLinger.l_linger=5;	// 容许逗留的时间为5s
		setsockopt(s,SOL_SOCKET,SO_LINGER,(const char*)&m_sLinger,sizeof(linger));

```

### fcntl文件描述符特性
```
fcntl函数可执行各种描述符控制操作，在网络编程中主要关注对套接字描述符的控制操作

#include <fcntl.h>
int fcntl(int fd, int cmd, int arg);
// fd	套接字描述符
// cmd	作用于该描述符上的命令
	F_GETFL		获取文件标志
	F_SETFL		设置文件标志
	F_SETOWN	指定用于接收SIGIO和SIGURG信号的套接字属主(进程或进程组)
				SIGIO:信号驱动IO，SIGURG:带外数据驱动
	F_GETOWN	返回套接字的当前属主
// arg	套接字标志
	O_NONBLOCK	非阻塞式IO
	O_ASYNC		信号驱动式IO
	进程ID		当cmd为F_SETOWN时，需设置进程ID(正值)或进程组ID(负值)
// 返回	成功则取决于cmd，失败返回-1

// 一般用法
int flags;
// 先获得flag
if ((flags = fcntl(fd, F_GETFL, 0)) < 0) error();

// 设置非阻塞
flags |= O_NONBLOCK;				
if (fcntl(fd, F_SETFL, flags) < 0) error();

// 关闭非阻塞
flags &= ~O_NONBLOCK;				
if (fcntl(fd, F_SETFL, flags) < 0) error();
```

### ioctl
```
ioctl是设备驱动程序中对设备的I/O通道进行管理的函数，ioctl是专门向驱动层发送或者接收指令

```

### 套接字错误
```
1、引发套接字关闭的错误
    (1) ETIMEOUT
        TCP请求未接受到响应，'超时'
    (2) ECONNRESET
        服务器主机崩溃重启后接收到客户端的请求，响应'RST'分节，客户端接收后设置套接字错误为该值
    (3) EHOSTUNREACH
        目的主机'不可达'

2、其它错误
    (1) EAGAIN
        通常发生在非阻塞I/O中，如果数据未准备好，I/O操作会返回这个错误，提示再试一次
    (2) EINTR
        表示系统调用被一个捕获的信号中断，发生该错误可以继续读写套接字


// 慢系统调用(slow system call)
此术语适用于那些可能永远阻塞的系统调用。永远阻塞的系统调用是指调用有可能永远无法返回，多数网络支持函数都属于这一类。如：若没有客户连接到服务器上，那么服务器的accept调用就没有返回的保证

3、EINTR
    (1) 错误的产生
        如果进程在一个慢系统调用(slow system call)中阻塞时，当捕获到某个信号且相应信号处理函数返回时，这个系统调用不再阻塞而是被中断，就会调用返回错误(-1)，且设置errno为EINTR(相应的错误描述为Interrupted system call)

    (2) 解决办法
        1)  重启被中断的系统调用
            accept、read、write、select、和open之类的函数来说，是可以进行重启的
            但connect函数是不能重启的，若connect函数返回一个EINTR错误的时候，我们不能再次调用它，否则将立即返回一个错误
        2) 安装信号时设置 SA_RESTART属性(该方法对有的系统调用无效)
        3) 忽略信号(让系统不产生信号中断)
```

### 套接字的连接
```
服务器启动后，调用socket, bind, listen, accept，并阻塞与accept调用

客户端调用socket，connect后，发起三次握手。握手完成后connect和accept均返回

// 连接管理
在连接之前需要设置socket选项，详见套接字选项章节
(1) TCP_MAXSEG
    该选项允许我们获取或设置TCP连接的最大分节大小(MSS)
(2) TCP_NODELAY
    开启该选项将禁止TCP的Nagle算法，默认情况下是启动的
```

### 套接字的终止
```
	
```

### 禁用Nagle
```
Nagle算法的本意是好的，就是要减少发送包的个数来提高网络效率，但这样一来，可能会延迟某些游戏数据包的发送，导致游戏网络延迟过高

TCP_NODELAY的选项来禁用Nagle算法
```

### 套接字IO
```
1、套接字超时的3种方法
    (1) alarm
        调用alarm，它在指定超时期满时产生SIGALRM信号
        此方法涉及信号处理，信号处理在不同实现上存在差异，而且可能干扰进程中现有的alarm信号
        > 缺点
            > 无法延长内核现有的超时期限(比如能比75小，但是不能更大)
            > 可被系统信号中断，返回EINTR错误。例如:
                connect连接中被ctrl+c信号中断，则返回EINTR
            > 在多线程中正确使用信号非常困难

    (2) epoll_wait
        epoll_wait可以设置超时
    (3) 套接字选项
        SO_RCVTIMEO(读)和SO_SNDTIMEO(写)套接字选项

2、read和write函数在套接字中的应用
    以下给出这两个系统IO函数的基本demo
    (1) read
        int readn(int fd, void* buf, int n) {
            char* ptr = buf;
            int nread = 0;
            int nLeft = n;
            while (nLeft > 0) {
                if ((nread = read(fd, ptr, nLeft)) < 0) {
                    if (errno == EINTR) {   // 被系统打断，重新调用read
                        nread = 0;
                    } else {
                        return -1;
                    }
                } else if (nread == 0) {
                    break;     // EOF
                }
                ptr += nread;
                nLeft -= nread;
            }
            return n-nLeft;
        }

    (2) write
        int writen(int fd, const void* buf, int n) {
            const char* ptr = buf;
            int nLeft = n;
            while (nLeft > 0) {
                if ((nwrite = write(fd, ptr, nLeft)) <= 0) {
                    if (nwrite < 0 && errno == EINTR) {
                        nwrite = 0;
                    } else {
                        return -1;
                    }
                }
                ptr += nwrite;
                nLeft -= nwrite;
            }
            return n;
        }

2、recv和send函数
    这两个函数比标准的read和write多了第4个参数
    #include <sys/socket.h>
    int recv(int sockfd, void* buf, size_t nbytes, int flag);
    int send(int sockfd, const void* buf, size_t nbytes, int flag);
    // sockfd   仅限于套接字
    // flag     默认0
        MSG_DONTROUTE(send)         绕过路由表查找
                                    告知内核目的主机在某个直接连接的本地网络上，因而无需执行路由表查找(临时)
                                    当然也可以给套接字开启SO_DONTROUTE选项(永久)

        MSG_DONTWAIT(recv, send)    本次操作非阻塞
                                    在无需设置套接字的非阻塞标志下，把单个I/O操作临时指定为非阻塞

        MSG_OOS(recv, send)         发送或接收带外数据

        MSG_PEEK(recv)              允许我们查看已经可读取的数据，而且系统不在recv或recvfrom返回后丢弃这些数据

        MSG_WAITALL(recv)           告知内核不要在尚未读入请求数目的字节之前让一个读操作返回(略)

3、readv和writev函数
    与标准的read和write不同在于
    readv和writev允许单个系统调用读入到或写出自一个或多个缓冲区，这些操作分别称为分散读和集中写
    来自读操作的输入数据"被分散"到多个应用缓冲区中
    来自多个应用缓冲区的输出数据则"被集中"提供给单个写操作
    #include <sys/uio.h>
    int readv(int fd, const struct iovec *iov, int iovcnt); 
    int writev(int fd, const struct iovec *iov, int iovcnt); 
    struct iovec {
        void *iov_base;
        size_t iov_len;
    }
    // fd   任何描述符

    writev函数从缓冲区中聚集输出数据的顺序是：iov[0]、iov[1]直至iov[iovcnt-1]
    readv函数则将读入的数据按同样的顺序散步到缓冲区中

4、recvmsg和sendmsg函数
    最通用的I/O函数
    #include <sys/socket.h>
    int recvmsg(int sockfd, struct msghdr* msg, int flags);
    int sendmsg(int sockfd, struct msghdr* msg, int flags);
    返回成功为读入或写出的字节数，出错返回-1

    struct msghdr {
        void *msg_name;             // protocol address
        socklen_t msg_namelen;      // size of protocol address
        struct iovec *msg_iov;
        int msg_iovlen;
        void *msg_control;
        socklen_t msg_controllen;
        int msg_flags;
    }

    待续。。。。。。。。。。。。。。。。。。。。。。

```

### 读写事件的发生
```
(1) 读事件
    1) 句柄从不可读变成可读，或者句柄写缓冲区有新的数据进来且超过SO_RCVLOWAT
    2) 产生事件的情况
        > socket有一个未清除的错误。如非阻塞的connect连接错误会使socket变成可读写状态
        > 非阻塞accept有新的连接进来
        > socket写对端关闭，read返回0
        > socket读缓冲区有新的数据进来且超过SO_RCVLOWAT

(2) 写事件
    1) 句柄从不可写变成可写，或者句柄写缓冲区有新的数据进来而且缓冲区水位高于SO_SNDLOWAT
    2) 产生事件的情况
        > socket有一个未清除的错误。例如非阻塞connect连接出错会导致socket变成可读可写状态
        > 非阻塞connect连接成功后端口状态会变成可写
        > socket读对端关闭，socket变成可写状态，产生SIGPIPE信号
        > socket写缓冲区有新的数据进来且超过SO_SNDLOWAT
    
在epoll中，读事件对应EPOLLIN，写事件对应EPOLLOUT
```

### 阻塞和非阻塞读写表现
```
(1) 对于阻塞socket
	read:	读缓冲区没有数据，发生阻塞
	write:	写缓冲区满了，发生阻塞
    如果返回-1代表网络出错了

(2) 对于非阻塞socket
    不能read或write时，就会返回-1，同时errno设置为EAGAIN(再试一次)
```

### 阻塞和非阻塞IO
```

```

### IO模型
```
阻塞式IO
非阻塞式IO
IO复用 --> Reactor模型
信号驱动式IO(SIGIO)
异步IO(AIO系列函数) --> Proactor模型
```

### IO复用
```
1、什么是IO复用
	复用就是为了解决有限资源和过多使用者的不平衡问题，即多数的工作/任务使用少数的资源
	I/O复用是一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪(一般是读就绪或写就绪)，能够通知程序进行相应的读写操作
	
	目前支持I/O复用的系统调用有select、poll、epoll，本质上这些I/O复用技术都是同步I/O，在读写事件就绪后需要进程自己负责进行读写

2、select
	select作为IO复用的开拓者，存在以下问题
	> 可协调fd数量和数值都不超过1024，无法实现高并发
	> 函数返回后需要轮询描述符集，寻找就绪描述符，效率不高
	> 用户态和内核态传递描述符结构时copy开销大

3、epoll
	epoll特点
	> 监视的描述符数量不受限制
	> I/O效率不会随着监视fd数量的增长而下降，epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的，只有就绪的fd才会执行回调函数
	> 用户态和内核态消息传递的开销小

               select  poll    epoll
操作方式		遍历	遍历	回调
底层实现		数组	链表	红黑树
IO效率			O(n)	O(n)	O(1)
最大连接数		1024	无限制	无限制
fd拷贝			fd集合	fd集合	准备好的fd集合

```

### epoll的相关概念
```
基于事件驱动的IO方式

https://yq.aliyun.com/articles/683282 [关于epoll的IO模型是同步异步的一次纠结过程]
https://segmentfault.com/a/1190000003063859 [Linux IO模式及 select、poll、epoll详解]
http://blog.chinaunix.net/uid-28541347-id-4273856.html [彻底学会使用epoll(一)——ET模式实现分析]
https://blog.csdn.net/dog250/article/details/80837278 [再谈Linux epoll惊群问题的原因和解决方案]
https://zhuanlan.zhihu.com/p/87843750 [深入理解IO复用之epoll]

1、底层实现
    (1) 两个重要结构体
        epoll底层实现最重要的两个数据结构 epitem和eventpoll
        epitem是和每个用户态监控IO的fd对应的
        eventpoll是用户态创建的管理所有被监控fd的结构
        1) eventpoll
            调用epoll_create方法时，内核会创建一个eventpoll对象，eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列
            epoll对象是进程与socket之间的中介，包含了监视队列和就绪列表

            struct eventpoll {
                ...
                wait_queue_head_t wq;           // sys_epoll_wait()使用的等待队列
                wait_queue_head_t poll_wait;    // file->poll()使用的等待队列

                struct list_head rdllist;       // 所有'准备就绪'的文件描述符列表
                struct rb_root rbr;             // 用于储存'已监控'fd的红黑树根节点
                
                struct epitem *ovflist;         // 当正在向用户空间传递事件时，此时如果有就绪事件，则会临时放到该队列，否则直接放到rdllist
                ...
            };

            每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件
            
            这些'事件'都会挂载在红黑树中(重复添加的事件就可以通过红黑树而高效的识别出来，红黑树的插入时间效率是O(logn)，其中n为元素个数)

        2) epitem
            监视队列和就绪列表的基本结构，每一个要监听的fd对应一个此结构体   
            
            所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说当相应的事件发生时会调用这个回调方法
            
            此回调方法在内核中叫ep_poll_callback，它会将发生的事件添加到rdlist双链表中

            在epoll中，对于每一个事件，都会建立一个epitem结构体，如下所示
            struct epitem{
                struct rb_node rbn;         //红黑树节点
                struct list_head rdllink;   //双向链表节点
                struct epitem *next; 
                struct epoll_filefd ffd;    //此条目引用的文件描述符信息
                struct eventpoll *ep;       //指向其所属的eventpoll对象
                struct epoll_event event;   //期待发生的事件类型
            } 
            当调用epoll_wait检查是否有事件发生时(所以epoll不是异步)，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可
            如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户

    (2) 两个重要数据结构
        1) rdllist双向链表
            连接就绪的epitem
        2) rbr红黑树
            存储监控的epitem
    
    (3) 调用过程
        > epoll_create创建一个epoll文件描述符，底层同时创建一个红黑树，和一个就绪链表
        > 红黑树存储所监控的文件描述符的节点数据，就绪链表存储就绪的文件描述符的节点数据
        > epoll_ctl将会添加新的描述符，首先判断是红黑树上是否有此文件描述符节点，如果有，则立即返回。如果没有，则在树干上插入新的节点，并且告知内核注册回调函数
        > 当接收到某个文件描述符过来数据时，那么内核将该节点插入到就绪链表里面
        > epoll_wait将会接收到消息，并且将数据拷贝到用户空间，清空链表
        > epoll_wait清空就绪链表之后会检查该文件描述符是哪一种模式，如果为LT模式，且必须该节点确实有事件未处理，那么就会把该节点重新放入到刚刚删除掉的且刚准备好的就绪链表，epoll_wait马上返回。ET模式不会检查，只会调用一次

2、工作模式
    https://www.jianshu.com/p/d3442ff24ba6 [epoll的LT和ET]
    http://kimi.it/515.html [Epoll在LT和ET模式下的读写方式]

    ET模式和FT模式
    使用ET的例子 -> nginx
    使用LT的例子 -> redis

    (1) ET(边缘触发)
        socket的接收缓冲区状态变化时触发读事件，即空的接收缓冲区刚接收到数据时触发读事件
        socket的发送缓冲区状态变化时触发写事件，即满的缓冲区刚空出空间时触发读事件
        边沿触发仅触发一次
        1) 特点
            > 句柄在发生读写事件时只会通知用户一次
            > ET模式主要关注fd从不可用到可用或者可用到不可用的情况
            > ET只支持非阻塞模式
        2) 一般用法
            > ET模式下读写操作要时用while循环，直到读/写够足够多的数据，或者读/写到返回EAGAIN
            > 写大块数据时，一次write操作不足以写完全部数据，或者在读大块数据时，应用层缓冲区数据太小，一次read操作不足以读完全部数据，应用层要么一直调用while循环一直IO到EGAIN，或者自己调用epoll_ctl手动触发ET响应

    (2) LT(水平触发，默认)
        socket接收缓冲区不为空，有数据可读，读事件一直触发
        socket发送缓冲区不满，可以继续写入数据，写事件一直触发
        水平触发会一直触发
        1) 特点
            > 只要句柄一直处于可用状态，就会一直通知用户
            > LT模式下，句柄读缓冲区被读空后，句柄会从可用转变未不可以用，这个时候不会通知用户。写缓冲区只要还没写满，就会一直通知用户
            > LT模式支持阻塞和非阻塞两种方式
            > LT下，应用层的业务逻辑比较简单，更不容易遗漏事件，更不容易出错。通常，在将数据写完后，我们会关闭句柄的写事件

    (3) ET vs LT
        LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，而ET模式仅在第一次返回

        > 当一个socket句柄上有事件时，内核会把该句柄插入准备就绪list链表
        > 调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表
        > 然后epoll_wait检查这些socket的模式
            > 如果是LT模式，并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，LT的句柄，只要它上面还有事件，epoll_wait每次都会返回
            > 如果是ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的
    
    (4) 使用建议
        对于监听的sockfd，最好使用水平触发模式，边缘触发模式会导致高并发情况下，有的客户端会连接不上。如果非要使用边缘触发，可以用while来循环accept()
        
        对于读写的 connfd，水平触发模式下，阻塞和非阻塞效果都一样，建议设置非阻塞
        
        对于读写的 connfd，边缘触发模式下，必须使用非阻塞 IO，并要求一次性地完整读写全部数据

// 注意
    epoll不能监听普通文件IO，应为epoll中会检查file是否配置了poll函数, 如果没有, 就会返回 EPERM
    ext等文件系统没有实现poll函数，所以不支持epoll操作

    poll 函数才能支持 epoll, 以下这些文件系统都支持 poll
    socket, eventfd, eventpoll, pipe, signalfd, timerfd, kmsg ...

```

### epoll相关操作
```
https://blog.csdn.net/ljx0305/article/details/4065058

// 用户能注册到epoll实例中的最大文件描述符的数量限制
/proc/sys/fs/epoll/max_user_watches

(1) int epoll_create(int size);
    创建一个epoll的句柄
    // size     用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值(不过目前size并不起作用了)
    // 返回     创建的epoll句柄
    // 注意
        当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽

(2) int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
    epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型
    // epfd     epoll_create()的返回值
    // op       表示动作，用三个宏来表示
        EPOLL_CTL_ADD   注册新的fd到epfd中
        EPOLL_CTL_MOD   修改已经注册的fd的监听事件
        EPOLL_CTL_DEL   从epfd中删除一个fd
    // fd       需要监听的fd
    // event    告知内核需要监听的事件
        // event结构
            struct epoll_event {
                __uint32_t events;      // epoll events
                epoll_data_t data;      // user data variable
            };
            
            typedef union epoll_data {  //用户数据载体
                void *ptr;              // 传进我们想要的内容，wait出来的时候，用我们自己的函数进行处理
                int fd;                 // 上面的fd
                __uint32_t u32;
                __uint64_t u64;
            } epoll_data_t;
            
        // epoll events操作
            EPOLLIN         表示对应的文件描述符可以读(包括对端SOCKET正常关闭)
            EPOLLOUT        表示对应的文件描述符可以写
            EPOLLPRI        表示对应的文件描述符有紧急的数据可读(表示有带外数据到来)
            EPOLLERR        表示对应的文件描述符发生错误
            EPOLLHUP        表示对应的文件描述符被挂断
            EPOLLET         将此描述符设为边缘触发(ET)模式
            EPOLLONESHOT    防止不同进程处理同一个socket事件
                A线程读完某socket上数据后开始处理这些数据，此时该socket上又有新数据可读，B线程被唤醒读新的数据，造成2个线程同时操作一个socket的局面 ，EPOLLONESHOT保证一个socket连接在任一时刻只被一个线程处理
            EPOLLEXCLUSIVE  linux内核4.5新加的，保证一个事件发生时候只有部分线程会被唤醒，缓解惊群问题

(3) int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
    等待epoll句柄上的I/O事件，最多返回maxevents个事件
    // epfd         同上
    // events       返回就绪事件的集合
    // maxevents    告之内核这个events有多大，这个 maxevents的值不能大于创建epoll_create()时的size
    // timeout      超时时间(毫秒)，0会立即返回，-1将不确定(永久阻塞)
    // 返回         大于0返回就绪事件数目，等于0表示超时

```

### epoll demo
```c++
// 这块在IO多线程模型中是reactor设计模型
// 没有考虑多线程的应用

#define MAX_EVENTS 10
struct epoll_event ev, events[MAX_EVENTS];
int listen_sock, conn_sock, nfds, epollfd;

/*
    socket();
    bind();
    listen();
*/

//创建一个描述符
epollfd = epoll_create(10);
if(epollfd == -1) {
    perror("epoll_create");
    exit(EXIT_FAILURE);
}

//添加监听描述符事件
ev.events = EPOLLIN;
ev.data.fd = listen_sock;
if(epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, &ev) == -1) {
    perror("epoll_ctl: listen_sock");
    exit(EXIT_FAILURE);
}

for(;;) {
    // 返回已经准备好的描述符事件数目
    nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1);
    if (nfds == -1) {
        perror("epoll_pwait");
        exit(EXIT_FAILURE);
    }

    // 遍历已经准备好的io事件
    for (n = 0; n < nfds; ++n) {
        if (events[n].data.fd == listen_sock) {
            // 主监听socket有新连接
            conn_sock = accept(listen_sock, (struct sockaddr *) &local, &addrlen);
            if (conn_sock == -1) {
                perror("accept");
                exit(EXIT_FAILURE);
            }
            // 设置非阻塞IO
            setnonblocking(conn_sock);
            ev.events = EPOLLIN | EPOLLET;
            ev.data.fd = conn_sock;
            // 添加新连接到epoll中
            if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, &ev) == -1) {
                perror("epoll_ctl: conn_sock");
                exit(EXIT_FAILURE);
            }
        } else {
            //已建立连接的可读写句柄
            do_use_fd(events[n].data.fd);
        }
    }
}

```

### 惊群
```
https://segmentfault.com/a/1190000039676522 [epoll惊群效应深度剖析]
https://www.cnblogs.com/sduzh/p/6810469.html [epoll惊群原因分析]
https://www.zhihu.com/question/24169490/answers/updated [Linux 3.x 中epoll的惊群问题？]
https://www.jianshu.com/p/21c3e5b99f4a [nginx如何解决惊群效应]
https://simpleyyt.com/2017/06/25/how-ngnix-solve-thundering-herd/ [Ngnix 是如何解决 epoll 惊群的]

惊群效应(thundering herd)是指多进/线程在同时阻塞等待同一个事件的时候(休眠状态)，如果等待的这个事件发生，那么他就会唤醒等待的所有进/线程，但是最终却只能有一个进/线程获得这个事件的"控制权"，对该事件进行处理，而其他进程(线程)获取"控制权"失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应

多个进程因为一个连接请求而被同时唤醒，称为惊群效应，在高并发情况下，大部分进程会无效地被唤醒然后因为抢占不到连接请求又重新进入睡眠，是会造成系统极大的性能损耗

1、accept惊群
    多个进程同时监听一个sockfd，每个进程都阻塞在accept，当一个新的连接到来时候，所有的进程都会被唤醒，但是其中只有一个进程会接受成功，其余皆失败，重新休眠

    在linux2.6版本以后，linux内核已经解决了accept()函数的"惊群"现象
    大概的处理方式是，当内核接收到一个客户连接后，只会唤醒等待队列上的第一个进程(线程)
    所以如果服务器采用accept阻塞调用方式，在最新的linux系统中已经没有'惊群效应'

2、epoll惊群
    (1) epoll存在惊群
        调用epoll_create创建一个epfd，然后多进/线程同时epoll_wait这个epfd事件
        当新的连接到来时，这些子进程全部(也可能是部分)被唤醒并处理事件
        但是只有一个子进程是可以处理连接，剩余的进程会获得一个EAGAIN信号，存在EAGAIN的进程不会继续读写，而错误退出

    (2) LT模式存在惊群
        epoll惊群只会在LT模式下发生
        在epoll的LT模式下，每次epoll_wait将readylist的event返回用户空间后，会将epi立刻再加入到ready_list里，而不像ET模式下是清空当前的ready_list。
        由于ready_list不为空了，当连接再次到来时，会再次调用waitqueue_active来唤醒epoll本身的等待队列，即其他线程的epoll_wait会返回, 造成惊群的现象

    (3) 解决办法
        1) accept_mutex锁
            每个worker都会先去抢自旋锁，只有抢占成功了，才把socket加入到epoll中，accept请求，然后释放锁。accept_mutex锁也有负载均衡的作用
            // 伪代码如下
            lock()
            epoll_wait(...);
            accept(...);
            unlock()

            效率低下，特别是在长连接时，一个进程长时间占用accept_mutex锁，使得其它进程得不到 accept 的机会
            accept_mutex的分配不均可能会导致进程饥饿

        2) EPOLLEXCLUSIVE(Nginx > 1.11.3)
            在epoll层面上，EPOLLEXCLUSIVE标识会保证一个事件发生时候只有部分(one or more)线程会被唤醒，一定程度上缓解了惊群效应
            不过任一时候只能有一个工作线程调用accept，限制了真正并行的吞吐量

        3) SO_REUSEPORT(Nginx > 1.9.1)
            SO_REUSEPORT是惊群最推荐的解决方法，每个worker都有自己的socket，这些socket都bind同一个端口。当新请求到来时，内核根据四元组信息进行负载均衡，非常高效
```

### 路由套接字
```
linux系统集成了路由功能，它包含相应的路由数据库可提供的路由信息，用户可以通过命令方式来增加、修改以及删除路由表中的项目，也可以只查看路由表的信息

在创建套接字时，可以通过指定参数AF_ROUTE域创建路由套接字，路由套接字可以访问内核中路由子系统的接口信息

路由套接字上支持 3 种类型的操作
	进程通过写到路由套接字向内核发送消息
	进程通过读入路由套接字接收来自内核的消息
	进程调用sysctl函数获取路由表或列出所有已配置的接口
```

### 带外数据
```

```

### ifref
```c++
// 网络设备信息

struct ifreq{
    char ifr_name[IFNAMSIZ];
    union{
        struct  sockaddr  ifru_addr;
        struct  sockaddr  ifru_dstaddr;
        struct  sockaddr  ifru_broadaddr;
        struct  sockaddr  ifru_netmask;
        struct  sockaddr  ifru_hwaddr;
        short   ifru_flags;
        int     ifru_metric;
        caddr_t ifru_data;
    }ifr_ifru;
};

// 打开一个socket
int fd = socket(AF_INET, SOCK_DGRAM, 0);

// 获得所有网路设备
struct ifreq devList[DEVICE_NUM];
struct ifconf ifg;
ifg.ifc_len = sizeof(devList);
ifg.ifc_buf = (char*)devList;
ioctl(fd, SIOCGIFCONF, (char*)&ifg);

// 网络设备数
size_t devNum = ifg.ifc_len / sizeof(struct ifreq);

// 设备1的name
struct ifreq* ifq = &devList[0];
ifq->ifr_name;

// 设备1的flag信息
ioctl(fd, SIOCGIFFLAGS, (char*)ifq);
uint32_t flag = ifq->ifr_flags; // IFF_UP IFF_BROADCAST IFF_RUNNING IFF_LOOPBACK

// 设备1的sockaddr信息
ioctl(fd, SIOCGIFADDR, (char*)ifq);
struct sockaddr_in * addr = (struct sockaddr_in*)(&ifq->ifr_addr);
const char* ip = inet_ntoa(addr->sin_addr);

// 设备1的mac信息
ioctl(fd, SIOCGIFHWADDR, (char*)ifq);
ifq->ifr_hwaddr.sa_data[0] -> [5]; // 6个char组成的mac id

```

### mongoose websocket 流程
```c++
// 初始化socketpair
mg_mgr_init()
    mg_mgr_init_opt()
        mg_socket_if_init()
            mg_socketpair() {
                ctl(0) = ctl[1] = socket(AF_INET, SOCK_STREAM, 0)
                bind()
            }

// poll等待
mg_socket_if_poll
    select()
        mg_mgr_handle_ctl_sock()
            ctl_msg = recv(ctl[1])
            send(ctl[1], ok)
            for_each (nc = mg_next(mgr, NULL) {
                ctl_msg.callback(nc, ...)
            }

// 发送消息 -> select
mg_broadcast
    send(ctl[0], ctl_msg)
    recv(ctl[0])


// 处理请求 mqtt发送
ctl_msg.callback(nc, ...)
    mg_send_websocket_frame(nc, WEBSOCKET_OP_TEXT, ptr, size)
```

### mongoose 架构
```
// 用户函数
cb()
    MG_EV_ACCEPT
    MG_EV_HTTP_MSG
    MG_EV_READ
    MG_EV_WRITE

// 监听注册
mg_http_listen()
    mg_listen()
        | listen
        | c->fn = cb 上面的用户函数
        | c->pfn = http_cb


for(;;)
    mg_mgr_poll()
        mg_iotest()
            for_each(所有连接)
                if (如果是新的连接(is_accept)) FD_SET到select中
            select(LISTEN, READ, ...)

        for_each(所有连接)
            cb(MG_EV_POLL)

            accept_conn()
                | c = accept
                | c <- is_accepted  // 先对新的连接赋予is_accepted，下次循环在添加到select中
                | cb(MG_EV_ACCEPT)

            connect_conn()
            
            read_conn()
                | recv
                | http_cb()
                    cb(MG_EV_HTTP_MSG)

            write_conn()
                | send
                | static_cb()
                |    fread
                | cb(MG_EV_WRITE)
```

### 已用文件描述符过多的处理办法
```c++
int listenfd;

int idleFd_ = open("/dev/null", O_RDONLY | O_CLOEXEC)

int connfd = accept(listenfd, NULL, NULL);

if (connfd >= 0) {

} else {
    // 如果已用文件描述符过多，accept会返回-1

    if (errno == EMFILE)
    {
      ::close(idleFd_);
      idleFd_ = ::accept(acceptSocket_.fd(), NULL, NULL);
      ::close(idleFd_);
      idleFd_ = ::open("/dev/null", O_RDONLY | O_CLOEXEC);
    }
}

// 上面注册的idleFd_先占用一个文件描述符
// 当前文件描述符过多，无法接收新的连接。但是由于我们采用LT模式，如果无法接收，可读事件会一直触发
// 那么在这个地方的处理机制就是，关掉之前创建的空闲idleFd_，然后去accept刚才失败的文件描述符，让这个事件不会一直触发，然后再关掉该文件描述符，重新将它设置为空文件描述符
```